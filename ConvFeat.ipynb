{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "overall-coral",
   "metadata": {},
   "outputs": [],
   "source": [
    "from fastai.vision.all import *\n",
    "from fastai.tabular.all import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "directed-reserve",
   "metadata": {},
   "outputs": [],
   "source": [
    "PATH = Path('../input/optiver-realized-volatility-prediction')\n",
    "\n",
    "# data_df = pd.read_parquet(PATH/'book_train_reindexed.parquet')\n",
    "\n",
    "# sizes = ['bid_size1', 'bid_size2', 'ask_size1', 'ask_size2']\n",
    "# for sz in sizes:\n",
    "#     data_df[sz] = data_df[sz]/ 1_000_000\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "supported-wings",
   "metadata": {},
   "outputs": [],
   "source": [
    "#train_df = pd.read_csv(PATH/'train_with_offset.csv')\n",
    "# data_df['wap'] = (data_df.bid_price1 * data_df.ask_size1 + data_df.ask_price1 * data_df.bid_size1) / (data_df.ask_size1 + data_df.bid_size1)\n",
    "\n",
    "# mean_wap =[]\n",
    "# for row in train_df.itertuples():\n",
    "#     df = data_df.iloc[row.offset:row.offset+600]\n",
    "#     mean_wap.append(df.wap.mean())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "convertible-being",
   "metadata": {},
   "outputs": [],
   "source": [
    "# train_df['mean_wap']=mean_wap\n",
    "# train_df.to_csv(PATH/'train_with_wap.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "certified-farmer",
   "metadata": {},
   "outputs": [],
   "source": [
    "# np_data = data_df[['bid_price1', 'ask_price1', 'bid_size1', 'ask_size1','bid_price2', 'ask_price2', 'bid_size2', 'ask_size2']].to_numpy()\n",
    "\n",
    "# del data_df\n",
    "\n",
    "# np_data = np_data.astype('float32')\n",
    "# torch_data = torch.tensor(np_data)\n",
    "\n",
    "# del np_data\n",
    "\n",
    "# torch.save(torch_data, PATH/'torch_data.pth')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "unusual-appointment",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 3.64 s, sys: 11.7 s, total: 15.3 s\n",
      "Wall time: 1min 56s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "torch_data = torch.load(PATH/'torch_data.pth')\n",
    "\n",
    "for i in (2,3,6,7): torch_data[:,i] *= 1_000_000"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "concerned-reasoning",
   "metadata": {},
   "outputs": [],
   "source": [
    "bid_price1, ask_price1, bid_size1, ask_size1,bid_price2, ask_price2, bid_size2, ask_size2 = [torch_data[:,i] for i in range(8)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "ancient-channel",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([8, 257359200])"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "wap = (bid_price1 * ask_size1 + ask_price1 * bid_size1)/(bid_size1 + ask_size1)\n",
    "wap2 = (bid_price2 * ask_size2 + ask_price2 * bid_size2)/(bid_size2 + ask_size2)\n",
    "wap_balance = wap - wap2\n",
    "price_spread =  (ask_price1 - bid_price1) / ((ask_price1 + bid_price1)/2)\n",
    "bid_spread = bid_price1 - bid_price2\n",
    "ask_spread = ask_price1 = ask_price2\n",
    "total_volume = ask_size1+ask_size2 +bid_size1 + bid_size2\n",
    "volume_imbalance = (ask_size1 + ask_size2) - (bid_size1 + bid_size2)\n",
    "feats= torch.stack([wap, wap2, wap_balance, price_spread, bid_spread, ask_spread, total_volume, volume_imbalance])\n",
    "feats.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "thermal-characteristic",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([257359200, 16])"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch_data = torch.cat([torch_data, feats.T], dim=1)\n",
    "torch_data.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "inappropriate-cigarette",
   "metadata": {},
   "outputs": [],
   "source": [
    "del bid_price1, ask_price1, bid_size1, ask_size1,bid_price2, ask_price2, bid_size2, ask_size2,wap, wap2, wap_balance, price_spread, bid_spread, ask_spread, total_volume, volume_imbalance,feats"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "religious-treasure",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([ 9.9968e-01,  1.0003e+00,  7.6999e+02,  7.6673e+02,  9.9948e-01,\n",
      "         1.0005e+00,  9.5934e+02,  9.2822e+02,  1.0000e+00,  1.0000e+00,\n",
      "         1.2723e-06,  6.6282e-04,  1.9751e-04,  1.0005e+00,  3.4243e+03,\n",
      "        -3.4377e+01]) tensor([3.6881e-03, 3.6871e-03, 5.3541e+03, 4.9549e+03, 3.7009e-03, 3.6991e-03,\n",
      "        6.6838e+03, 5.7353e+03, 3.6899e-03, 3.7075e-03, 5.1506e-04, 7.6403e-04,\n",
      "        2.5716e-04, 3.6991e-03, 2.0439e+04, 6.1751e+03])\n"
     ]
    }
   ],
   "source": [
    "means, stds = torch_data.mean(dim=0), torch_data.std(dim=0)\n",
    "print(means, stds)\n",
    "torch_data = (torch_data - means) / stds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "annoying-netscape",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_df = pd.read_csv(PATH/'train_with_wap.csv')\n",
    "\n",
    "train_ftrs = pd.read_csv(PATH/'train_with_ftrs.csv')\n",
    "train_ftrs.stock_id = train_ftrs.stock_id.astype('category')\n",
    "\n",
    "\n",
    "train_ftrs = train_ftrs.drop(['stock_id_target_enc', 'row_id', 'target'], axis=1)\n",
    "train_df = train_df.drop(['stock_id'], axis=1)\n",
    "\n",
    "train_df = pd.concat([train_df, train_ftrs], axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "recorded-beijing",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.9172531180048944 0.9971951138775056 0.9956492523836432\n",
      "0.9971951327900604 0.9984703256953886 0.9979708959569917\n",
      "0.9984703751656142 0.9991431543863876 0.9988451162091514\n",
      "0.9991431683337992 0.999616836138266 0.9993924853218963\n",
      "0.9996168497680276 1.0000141933324718 0.9998199779860442\n",
      "1.0000141975255898 1.0004091980748968 1.00020793857736\n",
      "1.0004091986325243 1.000876884050013 1.0006291062379966\n",
      "1.0008769297217603 1.0015230362748495 1.0011664016312456\n",
      "1.0015230933064834 1.002768629281105 1.0020189286622183\n",
      "1.0027686365447124 1.077264872305151 1.004330146304397\n"
     ]
    }
   ],
   "source": [
    "w = train_df.mean_wap.to_numpy()\n",
    "\n",
    "w =np.sort(w)\n",
    "\n",
    "bins = []\n",
    "\n",
    "step = (len(w)+9)//10\n",
    "for i in range(0, len(w), step):\n",
    "    j = min(i+step, len(w))\n",
    "    bins.append(w[j] if j< len(w) else 2)\n",
    "    print(w[i], w[j-1],np.median(w[i:j]) )\n",
    "\n",
    "wap_bin = np.digitize(train_df.mean_wap, bins)\n",
    "\n",
    "train_df['wap_bin']=wap_bin"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "systematic-sight",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.000105263 0.001436988 0.001175852\n",
      "0.001436991 0.001838282 0.0016455530000000001\n",
      "0.001838308 0.002210492 0.0020263215\n",
      "0.002210493 0.002604845 0.00240206\n",
      "0.002604847 0.003048064 0.002817908\n",
      "0.003048071 0.003585089 0.0033027735\n",
      "0.003585105 0.00428704 0.003905317\n",
      "0.004287042 0.005311553 0.0047378975\n",
      "0.005311559 0.007240946 0.0060829544999999995\n",
      "0.007240976 0.07032062 0.009453786499999998\n"
     ]
    }
   ],
   "source": [
    "w = train_df.target.to_numpy()\n",
    "\n",
    "w =np.sort(w)\n",
    "\n",
    "bins = []\n",
    "bin_med=[]\n",
    "step = (len(w)+9)//10\n",
    "for i in range(0, len(w), step):\n",
    "    j = min(i+step, len(w))\n",
    "    bins.append(w[j] if j< len(w) else 1)\n",
    "    bin_med.append(np.median(w[i:j]))\n",
    "    print(w[i], w[j-1],np.median(w[i:j]) )\n",
    "\n",
    "target_bin = np.digitize(train_df.target, bins)\n",
    "\n",
    "train_df['target_bin']=target_bin"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "stupid-wings",
   "metadata": {},
   "outputs": [],
   "source": [
    "class ResBlock(nn.Module):\n",
    "    def __init__(self, ch):\n",
    "        super().__init__()\n",
    "        self.layers = nn.Sequential(\n",
    "            nn.Conv1d(ch, ch, kernel_size = 5, padding = 2, padding_mode='replicate'),\n",
    "            nn.BatchNorm1d(ch),\n",
    "            nn.ReLU(),\n",
    "            nn.Conv1d(ch, ch, kernel_size = 5, padding = 2, padding_mode='replicate'),\n",
    "            nn.BatchNorm1d(ch),\n",
    "        )\n",
    "        \n",
    "    def forward(self, x):\n",
    "        res = self.layers(x) + x\n",
    "        res = F.relu(res)\n",
    "        return res"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "placed-wyoming",
   "metadata": {},
   "outputs": [],
   "source": [
    "EMB_DIM = 32\n",
    "\n",
    "class OptiverModel(nn.Module):\n",
    "    def __init__(self, conv_in, n_feat, conv_chan=32):\n",
    "        super().__init__()\n",
    "\n",
    "        conv_out = 4 * conv_chan\n",
    "        conv_layers = [nn.Conv1d(conv_in, conv_chan, kernel_size=1)]\n",
    "        for _ in range(8):\n",
    "            conv_layers += [ResBlock(conv_chan), ResBlock(conv_chan), ResBlock(conv_chan), ResBlock(conv_chan)\n",
    "                       , nn.AvgPool1d(2, padding=1),\n",
    "                      ]\n",
    "        conv_layers += [Flatten()]#, nn.Dropout(.1)]   \n",
    "        self.conv_layers = nn.Sequential(*conv_layers)\n",
    "        \n",
    "        self.stock_embedding = nn.Embedding(127,EMB_DIM )\n",
    "        self.emb_drop = nn.Dropout(.25)\n",
    "        class_inp =  EMB_DIM + n_feat  + conv_out\n",
    "        self.classifier = nn.Sequential(\n",
    "            LinBnDrop(class_inp, 400, p = 0, act = nn.ReLU()),\n",
    "            LinBnDrop(400, 200, p=.25, act = nn.ReLU()), \n",
    "            LinBnDrop(200, 1, p=0),\n",
    "            SigmoidRange(0, .1)\n",
    "        )\n",
    "        \n",
    "\n",
    "    def forward(self, stock_id, features, book_data):\n",
    "        book_feat = self.conv_layers(book_data)\n",
    "        stock_emb = self.emb_drop(self.stock_embedding(stock_id)).squeeze()\n",
    "        x = torch.cat([book_feat, stock_emb, features], dim=1)\n",
    "        res = self.classifier(x)\n",
    "        \n",
    "        return res"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "subtle-rabbit",
   "metadata": {},
   "outputs": [],
   "source": [
    "class SimpleModel(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.embeds = nn.Embedding(127, 23)\n",
    "        self.bn_embs = nn.BatchNorm1d(23)\n",
    "        self.layers = nn.Sequential(\n",
    "            LinBnDrop(49,400,act=nn.ReLU(), lin_first=True),\n",
    "            LinBnDrop(400,200, act=nn.ReLU(), lin_first=True),\n",
    "            LinBnDrop(200,1,bn=False),\n",
    "            SigmoidRange(0, .1)\n",
    "        )\n",
    "    def forward(self,  stock_id, feats):\n",
    "        stock_emb = self.embeds(stock_id.squeeze())\n",
    "        stock_emb = self.bn_embs(stock_emb)\n",
    "        x = torch.cat([stock_emb, feats], dim=1)\n",
    "        return self.layers(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "deluxe-criminal",
   "metadata": {},
   "outputs": [],
   "source": [
    "def rmspe(preds, targs):\n",
    "    x = (targs-preds)/targs\n",
    "    return (x**2).mean().sqrt()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "quarterly-williams",
   "metadata": {},
   "outputs": [],
   "source": [
    "def rmspe_loss(preds, targs, reduction='mean'):\n",
    "    assert reduction == 'mean'\n",
    "    x = (targs-preds)/targs\n",
    "    return (x**2).mean().sqrt()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "rental-clerk",
   "metadata": {},
   "outputs": [],
   "source": [
    "class MyProc(TabularProc):\n",
    "    def encodes(self, to):\n",
    "        print(type(to))\n",
    "        return to\n",
    "    def __getitem__(self,k):\n",
    "        print('getting', k)\n",
    "        return 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "western-planning",
   "metadata": {},
   "outputs": [],
   "source": [
    "class ReadBatch(ItemTransform):\n",
    "    def encodes(self, to):\n",
    "        book_offsets = torch.tensor(to['offset'].to_numpy()).long()\n",
    "        book_data = torch_data.view(-1,600,16)[book_offsets//600,:,:]\n",
    "        book_data = book_data.permute(0,2,1)\n",
    "        res = (tensor(to.cats).long(),tensor(to.conts).float(), book_data)        \n",
    "        res = res + (tensor(to.targ),)\n",
    "        if to.device is not None: res = to_device(res, to.device)\n",
    "        return res"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "martial-slope",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\u001b[0;31mInit signature:\u001b[0m \u001b[0mNormalize\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmean\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstd\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0maxes\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m2\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m3\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
       "\u001b[0;31mSource:\u001b[0m        \n",
       "\u001b[0;32mclass\u001b[0m \u001b[0mNormalize\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mDisplayedTransform\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\n",
       "\u001b[0;34m\u001b[0m    \u001b[0;34m\"Normalize/denorm batch of `TensorImage`\"\u001b[0m\u001b[0;34m\u001b[0m\n",
       "\u001b[0;34m\u001b[0m    \u001b[0mparameters\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0morder\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mL\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'mean'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'std'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m99\u001b[0m\u001b[0;34m\u001b[0m\n",
       "\u001b[0;34m\u001b[0m    \u001b[0;32mdef\u001b[0m \u001b[0m__init__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmean\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstd\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0maxes\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m2\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m3\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mstore_attr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\n",
       "\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\n",
       "\u001b[0;34m\u001b[0m    \u001b[0;34m@\u001b[0m\u001b[0mclassmethod\u001b[0m\u001b[0;34m\u001b[0m\n",
       "\u001b[0;34m\u001b[0m    \u001b[0;32mdef\u001b[0m \u001b[0mfrom_stats\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcls\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmean\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstd\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdim\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mndim\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m4\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcuda\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0;32mreturn\u001b[0m \u001b[0mcls\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0mbroadcast_vec\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdim\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mndim\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmean\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstd\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcuda\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mcuda\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\n",
       "\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\n",
       "\u001b[0;34m\u001b[0m    \u001b[0;32mdef\u001b[0m \u001b[0msetups\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdl\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0mDataLoader\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\n",
       "\u001b[0;34m\u001b[0m        \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmean\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstd\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\n",
       "\u001b[0;34m\u001b[0m            \u001b[0mx\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0m_\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdl\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mone_batch\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\n",
       "\u001b[0;34m\u001b[0m            \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmean\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstd\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmean\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0maxes\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkeepdim\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstd\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0maxes\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkeepdim\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m+\u001b[0m\u001b[0;36m1e-7\u001b[0m\u001b[0;34m\u001b[0m\n",
       "\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\n",
       "\u001b[0;34m\u001b[0m    \u001b[0;32mdef\u001b[0m \u001b[0mencodes\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0mTensorImage\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0;32mreturn\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmean\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m/\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstd\u001b[0m\u001b[0;34m\u001b[0m\n",
       "\u001b[0;34m\u001b[0m    \u001b[0;32mdef\u001b[0m \u001b[0mdecodes\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0mTensorImage\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\n",
       "\u001b[0;34m\u001b[0m        \u001b[0mf\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mto_cpu\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdevice\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtype\u001b[0m\u001b[0;34m==\u001b[0m\u001b[0;34m'cpu'\u001b[0m \u001b[0;32melse\u001b[0m \u001b[0mnoop\u001b[0m\u001b[0;34m\u001b[0m\n",
       "\u001b[0;34m\u001b[0m        \u001b[0;32mreturn\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0mf\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstd\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mf\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmean\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\n",
       "\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\n",
       "\u001b[0;34m\u001b[0m    \u001b[0m_docs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mdict\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mencodes\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m\"Normalize batch\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdecodes\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m\"Denormalize batch\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
       "\u001b[0;31mFile:\u001b[0m           /mnt/dysk25/repos/fastai/fastai/data/transforms.py\n",
       "\u001b[0;31mType:\u001b[0m           _TfmMeta\n",
       "\u001b[0;31mSubclasses:\u001b[0m     \n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "Normalize??"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "incident-pursuit",
   "metadata": {},
   "outputs": [],
   "source": [
    "@Normalize\n",
    "def setups(self, to:Tabular):\n",
    "    store_attr(but='to', means=dict(getattr(to, 'train', to).conts.mean()),\n",
    "               stds=dict(getattr(to, 'train', to).conts.std(ddof=0)+1e-7))\n",
    "    return self(to)\n",
    "\n",
    "@Normalize\n",
    "def encodes(self, to:Tabular):\n",
    "    print('Normalizing')\n",
    "    to.conts = (to.conts-self.means) / self.stds\n",
    "    return to\n",
    "\n",
    "@Normalize\n",
    "def decodes(self, to:Tabular):\n",
    "    to.conts = (to.conts*self.stds ) + self.means\n",
    "    return to"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "authorized-lesson",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Normalizing\n"
     ]
    }
   ],
   "source": [
    "\n",
    "cont_nn,cat_nn = cont_cat_split(train_ftrs, max_card=9000, dep_var='target')\n",
    "procs_nn = [Categorify, FillMissing, Normalize]\n",
    "to_nn = TabularPandas(train_df, procs_nn, cat_nn, cont_nn,\n",
    "                      splits=ColSplitter()(train_df), y_names='target')\n",
    "dls = to_nn.dataloaders(1024, after_batch = ReadBatch)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "pacific-chest",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(torch.Size([1024, 1]),\n",
       " torch.Size([1024, 26]),\n",
       " torch.Size([1024, 16, 600]),\n",
       " torch.Size([1024, 1]))"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "bx1, bx2,bx3, by = dls.one_batch()\n",
    "\n",
    "bx1.shape, bx2.shape, bx3.shape, by.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "falling-difficulty",
   "metadata": {},
   "outputs": [],
   "source": [
    "_=dls.valid.one_batch()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "decreased-waterproof",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = OptiverModel(16, 26).cuda()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "protected-collection",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([1024, 1])"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model(bx1.cuda(),bx2.cuda(), bx3.cuda()).shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "buried-oakland",
   "metadata": {},
   "outputs": [],
   "source": [
    "class ResnetModel(nn.Module):\n",
    "    def __init__(self):\n",
    "        self.conv_layers = None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "metric-syracuse",
   "metadata": {},
   "outputs": [],
   "source": [
    "def split_2way(model):\n",
    "    #return L(params(model.initial_conv)+params(model.conv_layers), params(model.classifier))\n",
    "    return L(params(model.conv_layers), params(model.classifier)+params(model.stock_embedding))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "turned-earthquake",
   "metadata": {},
   "outputs": [],
   "source": [
    "conv_layers = torch.load('resnet_feat.pth').conv_layers\n",
    "\n",
    "model = OptiverModel(16, 26)\n",
    "model.conv_layers = conv_layers\n",
    "\n",
    "#learn = Learner(dls, ConvModel(),loss_func=mspe_loss, metrics=AccumMetric(rmspe))\n",
    "learn = Learner(dls,model, loss_func=rmspe_loss, splitter = split_2way, metrics=AccumMetric(rmspe))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "extended-relaxation",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: left;\">\n",
       "      <th>epoch</th>\n",
       "      <th>train_loss</th>\n",
       "      <th>valid_loss</th>\n",
       "      <th>rmspe</th>\n",
       "      <th>time</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>0</td>\n",
       "      <td>7.997128</td>\n",
       "      <td>8.498618</td>\n",
       "      <td>8.541858</td>\n",
       "      <td>00:35</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>5.683980</td>\n",
       "      <td>5.037484</td>\n",
       "      <td>5.097029</td>\n",
       "      <td>00:35</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>4.316352</td>\n",
       "      <td>3.698126</td>\n",
       "      <td>3.788697</td>\n",
       "      <td>00:35</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>3.221061</td>\n",
       "      <td>2.500652</td>\n",
       "      <td>2.604899</td>\n",
       "      <td>00:35</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4</td>\n",
       "      <td>2.302841</td>\n",
       "      <td>1.776741</td>\n",
       "      <td>1.868098</td>\n",
       "      <td>00:35</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>5</td>\n",
       "      <td>1.703205</td>\n",
       "      <td>1.348328</td>\n",
       "      <td>1.429101</td>\n",
       "      <td>00:35</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>6</td>\n",
       "      <td>1.069030</td>\n",
       "      <td>0.922054</td>\n",
       "      <td>0.961660</td>\n",
       "      <td>00:35</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>7</td>\n",
       "      <td>0.824780</td>\n",
       "      <td>0.880993</td>\n",
       "      <td>0.907036</td>\n",
       "      <td>00:35</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>8</td>\n",
       "      <td>0.635889</td>\n",
       "      <td>0.698549</td>\n",
       "      <td>0.728229</td>\n",
       "      <td>00:35</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>9</td>\n",
       "      <td>0.359029</td>\n",
       "      <td>0.362370</td>\n",
       "      <td>0.375438</td>\n",
       "      <td>00:34</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>10</td>\n",
       "      <td>0.284881</td>\n",
       "      <td>0.264471</td>\n",
       "      <td>0.268617</td>\n",
       "      <td>00:34</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>11</td>\n",
       "      <td>0.265505</td>\n",
       "      <td>0.260366</td>\n",
       "      <td>0.263606</td>\n",
       "      <td>00:35</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>12</td>\n",
       "      <td>0.253297</td>\n",
       "      <td>0.247395</td>\n",
       "      <td>0.250211</td>\n",
       "      <td>00:35</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>13</td>\n",
       "      <td>0.262797</td>\n",
       "      <td>0.241809</td>\n",
       "      <td>0.244933</td>\n",
       "      <td>00:35</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>14</td>\n",
       "      <td>0.249930</td>\n",
       "      <td>0.243072</td>\n",
       "      <td>0.245673</td>\n",
       "      <td>00:35</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>15</td>\n",
       "      <td>0.260618</td>\n",
       "      <td>0.246651</td>\n",
       "      <td>0.250084</td>\n",
       "      <td>00:36</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>16</td>\n",
       "      <td>0.251566</td>\n",
       "      <td>0.240886</td>\n",
       "      <td>0.243897</td>\n",
       "      <td>00:35</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>17</td>\n",
       "      <td>0.250305</td>\n",
       "      <td>0.243184</td>\n",
       "      <td>0.246365</td>\n",
       "      <td>00:35</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>18</td>\n",
       "      <td>0.246110</td>\n",
       "      <td>0.245545</td>\n",
       "      <td>0.248460</td>\n",
       "      <td>00:35</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>19</td>\n",
       "      <td>0.263777</td>\n",
       "      <td>0.241648</td>\n",
       "      <td>0.244404</td>\n",
       "      <td>00:35</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>20</td>\n",
       "      <td>0.249931</td>\n",
       "      <td>0.239017</td>\n",
       "      <td>0.242019</td>\n",
       "      <td>00:35</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>21</td>\n",
       "      <td>0.256132</td>\n",
       "      <td>0.234588</td>\n",
       "      <td>0.237291</td>\n",
       "      <td>00:35</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>22</td>\n",
       "      <td>0.245711</td>\n",
       "      <td>0.276183</td>\n",
       "      <td>0.314758</td>\n",
       "      <td>00:35</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>23</td>\n",
       "      <td>0.243876</td>\n",
       "      <td>0.239414</td>\n",
       "      <td>0.242575</td>\n",
       "      <td>00:35</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>24</td>\n",
       "      <td>0.243745</td>\n",
       "      <td>0.240411</td>\n",
       "      <td>0.243429</td>\n",
       "      <td>00:35</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>25</td>\n",
       "      <td>0.252523</td>\n",
       "      <td>0.234051</td>\n",
       "      <td>0.236863</td>\n",
       "      <td>00:34</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>26</td>\n",
       "      <td>0.246394</td>\n",
       "      <td>0.238145</td>\n",
       "      <td>0.240904</td>\n",
       "      <td>00:34</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>27</td>\n",
       "      <td>0.290650</td>\n",
       "      <td>0.234618</td>\n",
       "      <td>0.237128</td>\n",
       "      <td>00:34</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>28</td>\n",
       "      <td>0.275705</td>\n",
       "      <td>0.232826</td>\n",
       "      <td>0.235834</td>\n",
       "      <td>00:34</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>29</td>\n",
       "      <td>0.246085</td>\n",
       "      <td>0.232296</td>\n",
       "      <td>0.235786</td>\n",
       "      <td>00:34</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>30</td>\n",
       "      <td>0.242843</td>\n",
       "      <td>0.234445</td>\n",
       "      <td>0.237276</td>\n",
       "      <td>00:34</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>31</td>\n",
       "      <td>0.253023</td>\n",
       "      <td>0.240381</td>\n",
       "      <td>0.254942</td>\n",
       "      <td>00:34</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>32</td>\n",
       "      <td>0.246100</td>\n",
       "      <td>0.235640</td>\n",
       "      <td>0.238634</td>\n",
       "      <td>00:34</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>33</td>\n",
       "      <td>0.246773</td>\n",
       "      <td>0.238716</td>\n",
       "      <td>0.241398</td>\n",
       "      <td>00:34</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>34</td>\n",
       "      <td>0.249311</td>\n",
       "      <td>0.232273</td>\n",
       "      <td>0.235018</td>\n",
       "      <td>00:34</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>35</td>\n",
       "      <td>0.265723</td>\n",
       "      <td>0.235813</td>\n",
       "      <td>0.238424</td>\n",
       "      <td>00:34</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>36</td>\n",
       "      <td>0.253332</td>\n",
       "      <td>0.232122</td>\n",
       "      <td>0.235095</td>\n",
       "      <td>00:34</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>37</td>\n",
       "      <td>0.239266</td>\n",
       "      <td>0.235231</td>\n",
       "      <td>0.238008</td>\n",
       "      <td>00:34</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>38</td>\n",
       "      <td>0.253128</td>\n",
       "      <td>0.230453</td>\n",
       "      <td>0.233406</td>\n",
       "      <td>00:34</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>39</td>\n",
       "      <td>0.237981</td>\n",
       "      <td>0.229182</td>\n",
       "      <td>0.232051</td>\n",
       "      <td>00:34</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: left;\">\n",
       "      <th>epoch</th>\n",
       "      <th>train_loss</th>\n",
       "      <th>valid_loss</th>\n",
       "      <th>rmspe</th>\n",
       "      <th>time</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>0</td>\n",
       "      <td>0.239691</td>\n",
       "      <td>0.230648</td>\n",
       "      <td>0.233576</td>\n",
       "      <td>00:45</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>0.238779</td>\n",
       "      <td>0.229809</td>\n",
       "      <td>0.232953</td>\n",
       "      <td>00:45</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>0.238798</td>\n",
       "      <td>0.245833</td>\n",
       "      <td>0.275786</td>\n",
       "      <td>00:45</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>0.238319</td>\n",
       "      <td>0.230882</td>\n",
       "      <td>0.233795</td>\n",
       "      <td>00:45</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4</td>\n",
       "      <td>0.242525</td>\n",
       "      <td>0.230494</td>\n",
       "      <td>0.233348</td>\n",
       "      <td>00:45</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>5</td>\n",
       "      <td>0.237314</td>\n",
       "      <td>0.233066</td>\n",
       "      <td>0.236073</td>\n",
       "      <td>00:45</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>6</td>\n",
       "      <td>0.236379</td>\n",
       "      <td>0.231086</td>\n",
       "      <td>0.234197</td>\n",
       "      <td>00:45</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>7</td>\n",
       "      <td>0.235916</td>\n",
       "      <td>0.230920</td>\n",
       "      <td>0.233745</td>\n",
       "      <td>00:45</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>8</td>\n",
       "      <td>0.236314</td>\n",
       "      <td>0.232918</td>\n",
       "      <td>0.236044</td>\n",
       "      <td>00:45</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>9</td>\n",
       "      <td>0.236832</td>\n",
       "      <td>0.231911</td>\n",
       "      <td>0.235006</td>\n",
       "      <td>00:45</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>10</td>\n",
       "      <td>0.238835</td>\n",
       "      <td>0.231729</td>\n",
       "      <td>0.234520</td>\n",
       "      <td>00:45</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>11</td>\n",
       "      <td>0.239791</td>\n",
       "      <td>0.233029</td>\n",
       "      <td>0.235634</td>\n",
       "      <td>00:45</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>12</td>\n",
       "      <td>0.236876</td>\n",
       "      <td>0.232599</td>\n",
       "      <td>0.235188</td>\n",
       "      <td>00:45</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>13</td>\n",
       "      <td>0.237597</td>\n",
       "      <td>0.232384</td>\n",
       "      <td>0.235275</td>\n",
       "      <td>00:45</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>14</td>\n",
       "      <td>0.237706</td>\n",
       "      <td>0.230496</td>\n",
       "      <td>0.233692</td>\n",
       "      <td>00:45</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>15</td>\n",
       "      <td>0.235853</td>\n",
       "      <td>0.231653</td>\n",
       "      <td>0.234335</td>\n",
       "      <td>00:45</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>16</td>\n",
       "      <td>0.237913</td>\n",
       "      <td>0.230259</td>\n",
       "      <td>0.233167</td>\n",
       "      <td>00:45</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>17</td>\n",
       "      <td>0.235698</td>\n",
       "      <td>0.231083</td>\n",
       "      <td>0.234061</td>\n",
       "      <td>00:45</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>18</td>\n",
       "      <td>0.234864</td>\n",
       "      <td>0.230879</td>\n",
       "      <td>0.234322</td>\n",
       "      <td>00:45</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>19</td>\n",
       "      <td>0.235131</td>\n",
       "      <td>0.230028</td>\n",
       "      <td>0.233139</td>\n",
       "      <td>00:45</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>20</td>\n",
       "      <td>0.234123</td>\n",
       "      <td>0.228970</td>\n",
       "      <td>0.231798</td>\n",
       "      <td>00:45</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>21</td>\n",
       "      <td>0.234318</td>\n",
       "      <td>0.231463</td>\n",
       "      <td>0.234248</td>\n",
       "      <td>00:45</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>22</td>\n",
       "      <td>0.234924</td>\n",
       "      <td>0.230657</td>\n",
       "      <td>0.233448</td>\n",
       "      <td>00:45</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>23</td>\n",
       "      <td>0.234901</td>\n",
       "      <td>0.229618</td>\n",
       "      <td>0.233614</td>\n",
       "      <td>00:45</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>24</td>\n",
       "      <td>0.233950</td>\n",
       "      <td>0.249778</td>\n",
       "      <td>0.318710</td>\n",
       "      <td>00:45</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>25</td>\n",
       "      <td>0.233553</td>\n",
       "      <td>0.226526</td>\n",
       "      <td>0.229691</td>\n",
       "      <td>00:45</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>26</td>\n",
       "      <td>0.234070</td>\n",
       "      <td>0.230943</td>\n",
       "      <td>0.236971</td>\n",
       "      <td>00:45</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>27</td>\n",
       "      <td>0.232460</td>\n",
       "      <td>0.229261</td>\n",
       "      <td>0.232284</td>\n",
       "      <td>00:45</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>28</td>\n",
       "      <td>0.234483</td>\n",
       "      <td>0.230919</td>\n",
       "      <td>0.233365</td>\n",
       "      <td>00:45</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>29</td>\n",
       "      <td>0.232707</td>\n",
       "      <td>0.229600</td>\n",
       "      <td>0.232270</td>\n",
       "      <td>00:45</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>30</td>\n",
       "      <td>0.232208</td>\n",
       "      <td>0.230024</td>\n",
       "      <td>0.233570</td>\n",
       "      <td>00:45</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>31</td>\n",
       "      <td>0.231748</td>\n",
       "      <td>0.250457</td>\n",
       "      <td>0.299234</td>\n",
       "      <td>00:45</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>32</td>\n",
       "      <td>0.231800</td>\n",
       "      <td>0.239974</td>\n",
       "      <td>0.278049</td>\n",
       "      <td>00:45</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>33</td>\n",
       "      <td>0.231870</td>\n",
       "      <td>0.259310</td>\n",
       "      <td>0.379210</td>\n",
       "      <td>00:45</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>34</td>\n",
       "      <td>0.232254</td>\n",
       "      <td>0.231633</td>\n",
       "      <td>0.234458</td>\n",
       "      <td>00:45</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>35</td>\n",
       "      <td>0.231779</td>\n",
       "      <td>0.279291</td>\n",
       "      <td>0.436967</td>\n",
       "      <td>00:45</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>36</td>\n",
       "      <td>0.230672</td>\n",
       "      <td>0.228703</td>\n",
       "      <td>0.231803</td>\n",
       "      <td>00:45</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>37</td>\n",
       "      <td>0.231545</td>\n",
       "      <td>0.227908</td>\n",
       "      <td>0.230657</td>\n",
       "      <td>00:45</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>38</td>\n",
       "      <td>0.231145</td>\n",
       "      <td>0.227090</td>\n",
       "      <td>0.229968</td>\n",
       "      <td>00:45</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>39</td>\n",
       "      <td>0.231405</td>\n",
       "      <td>0.227037</td>\n",
       "      <td>0.230045</td>\n",
       "      <td>00:45</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>40</td>\n",
       "      <td>0.230971</td>\n",
       "      <td>0.227634</td>\n",
       "      <td>0.231208</td>\n",
       "      <td>00:45</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>41</td>\n",
       "      <td>0.230186</td>\n",
       "      <td>0.233183</td>\n",
       "      <td>0.236342</td>\n",
       "      <td>00:45</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>42</td>\n",
       "      <td>0.231919</td>\n",
       "      <td>0.226821</td>\n",
       "      <td>0.230083</td>\n",
       "      <td>00:45</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>43</td>\n",
       "      <td>0.230140</td>\n",
       "      <td>0.230154</td>\n",
       "      <td>0.232878</td>\n",
       "      <td>00:45</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>44</td>\n",
       "      <td>0.228582</td>\n",
       "      <td>0.232576</td>\n",
       "      <td>0.236774</td>\n",
       "      <td>00:45</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>45</td>\n",
       "      <td>0.229630</td>\n",
       "      <td>0.227062</td>\n",
       "      <td>0.229941</td>\n",
       "      <td>00:45</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>46</td>\n",
       "      <td>0.230852</td>\n",
       "      <td>0.231894</td>\n",
       "      <td>0.234675</td>\n",
       "      <td>00:45</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>47</td>\n",
       "      <td>0.229732</td>\n",
       "      <td>0.229495</td>\n",
       "      <td>0.232441</td>\n",
       "      <td>00:45</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>48</td>\n",
       "      <td>0.230877</td>\n",
       "      <td>0.235870</td>\n",
       "      <td>0.250224</td>\n",
       "      <td>00:45</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>49</td>\n",
       "      <td>0.228516</td>\n",
       "      <td>0.227876</td>\n",
       "      <td>0.231300</td>\n",
       "      <td>00:45</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>50</td>\n",
       "      <td>0.227733</td>\n",
       "      <td>0.226991</td>\n",
       "      <td>0.230212</td>\n",
       "      <td>00:45</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>51</td>\n",
       "      <td>0.229223</td>\n",
       "      <td>0.259885</td>\n",
       "      <td>0.322643</td>\n",
       "      <td>00:45</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>52</td>\n",
       "      <td>0.229549</td>\n",
       "      <td>0.228396</td>\n",
       "      <td>0.233025</td>\n",
       "      <td>00:45</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>53</td>\n",
       "      <td>0.227967</td>\n",
       "      <td>0.230903</td>\n",
       "      <td>0.244379</td>\n",
       "      <td>00:45</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>54</td>\n",
       "      <td>0.229274</td>\n",
       "      <td>0.227780</td>\n",
       "      <td>0.231210</td>\n",
       "      <td>00:45</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>55</td>\n",
       "      <td>0.230734</td>\n",
       "      <td>0.232876</td>\n",
       "      <td>0.245749</td>\n",
       "      <td>00:45</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>56</td>\n",
       "      <td>0.230560</td>\n",
       "      <td>0.226312</td>\n",
       "      <td>0.229080</td>\n",
       "      <td>00:45</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>57</td>\n",
       "      <td>0.230124</td>\n",
       "      <td>0.229385</td>\n",
       "      <td>0.235940</td>\n",
       "      <td>00:45</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>58</td>\n",
       "      <td>0.227872</td>\n",
       "      <td>0.227683</td>\n",
       "      <td>0.230665</td>\n",
       "      <td>00:45</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>59</td>\n",
       "      <td>0.226205</td>\n",
       "      <td>0.228014</td>\n",
       "      <td>0.231581</td>\n",
       "      <td>00:45</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>60</td>\n",
       "      <td>0.227939</td>\n",
       "      <td>0.226315</td>\n",
       "      <td>0.229815</td>\n",
       "      <td>00:45</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>61</td>\n",
       "      <td>0.228581</td>\n",
       "      <td>0.229188</td>\n",
       "      <td>0.234056</td>\n",
       "      <td>00:45</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>62</td>\n",
       "      <td>0.231407</td>\n",
       "      <td>0.232748</td>\n",
       "      <td>0.242900</td>\n",
       "      <td>00:45</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>63</td>\n",
       "      <td>0.227818</td>\n",
       "      <td>0.226639</td>\n",
       "      <td>0.229326</td>\n",
       "      <td>00:45</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>64</td>\n",
       "      <td>0.226942</td>\n",
       "      <td>0.232256</td>\n",
       "      <td>0.240950</td>\n",
       "      <td>00:45</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>65</td>\n",
       "      <td>0.229923</td>\n",
       "      <td>0.227177</td>\n",
       "      <td>0.229679</td>\n",
       "      <td>00:45</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>66</td>\n",
       "      <td>0.229927</td>\n",
       "      <td>0.225086</td>\n",
       "      <td>0.227716</td>\n",
       "      <td>00:45</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>67</td>\n",
       "      <td>0.228120</td>\n",
       "      <td>0.226828</td>\n",
       "      <td>0.229481</td>\n",
       "      <td>00:45</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>68</td>\n",
       "      <td>0.226662</td>\n",
       "      <td>0.226872</td>\n",
       "      <td>0.229996</td>\n",
       "      <td>00:45</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>69</td>\n",
       "      <td>0.228023</td>\n",
       "      <td>0.251033</td>\n",
       "      <td>0.323361</td>\n",
       "      <td>00:45</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>70</td>\n",
       "      <td>0.227138</td>\n",
       "      <td>0.257404</td>\n",
       "      <td>0.375031</td>\n",
       "      <td>00:45</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>71</td>\n",
       "      <td>0.228118</td>\n",
       "      <td>0.227423</td>\n",
       "      <td>0.230192</td>\n",
       "      <td>00:45</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>72</td>\n",
       "      <td>0.227059</td>\n",
       "      <td>0.229203</td>\n",
       "      <td>0.232624</td>\n",
       "      <td>00:45</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>73</td>\n",
       "      <td>0.227304</td>\n",
       "      <td>0.227502</td>\n",
       "      <td>0.230025</td>\n",
       "      <td>00:45</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>74</td>\n",
       "      <td>0.227360</td>\n",
       "      <td>0.228090</td>\n",
       "      <td>0.230748</td>\n",
       "      <td>00:45</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>75</td>\n",
       "      <td>0.228622</td>\n",
       "      <td>0.245903</td>\n",
       "      <td>0.300003</td>\n",
       "      <td>00:45</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>76</td>\n",
       "      <td>0.228246</td>\n",
       "      <td>0.226514</td>\n",
       "      <td>0.229024</td>\n",
       "      <td>00:45</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>77</td>\n",
       "      <td>0.227327</td>\n",
       "      <td>0.231485</td>\n",
       "      <td>0.238198</td>\n",
       "      <td>00:45</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>78</td>\n",
       "      <td>0.227344</td>\n",
       "      <td>0.236526</td>\n",
       "      <td>0.253002</td>\n",
       "      <td>00:45</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>79</td>\n",
       "      <td>0.228023</td>\n",
       "      <td>0.227222</td>\n",
       "      <td>0.229860</td>\n",
       "      <td>00:45</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>80</td>\n",
       "      <td>0.227320</td>\n",
       "      <td>0.229364</td>\n",
       "      <td>0.232657</td>\n",
       "      <td>00:45</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>81</td>\n",
       "      <td>0.228764</td>\n",
       "      <td>0.231838</td>\n",
       "      <td>0.236097</td>\n",
       "      <td>00:45</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>82</td>\n",
       "      <td>0.226836</td>\n",
       "      <td>0.234670</td>\n",
       "      <td>0.244784</td>\n",
       "      <td>00:45</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>83</td>\n",
       "      <td>0.229427</td>\n",
       "      <td>0.227709</td>\n",
       "      <td>0.231319</td>\n",
       "      <td>00:45</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>84</td>\n",
       "      <td>0.227470</td>\n",
       "      <td>0.227173</td>\n",
       "      <td>0.230396</td>\n",
       "      <td>00:45</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>85</td>\n",
       "      <td>0.228045</td>\n",
       "      <td>0.248077</td>\n",
       "      <td>0.299267</td>\n",
       "      <td>00:46</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>86</td>\n",
       "      <td>0.229437</td>\n",
       "      <td>0.248996</td>\n",
       "      <td>0.308587</td>\n",
       "      <td>00:47</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>87</td>\n",
       "      <td>0.227057</td>\n",
       "      <td>0.256230</td>\n",
       "      <td>0.370303</td>\n",
       "      <td>00:47</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>88</td>\n",
       "      <td>0.225170</td>\n",
       "      <td>0.250895</td>\n",
       "      <td>0.318204</td>\n",
       "      <td>00:47</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>89</td>\n",
       "      <td>0.227235</td>\n",
       "      <td>0.258550</td>\n",
       "      <td>0.374802</td>\n",
       "      <td>00:47</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>90</td>\n",
       "      <td>0.228462</td>\n",
       "      <td>0.239791</td>\n",
       "      <td>0.268539</td>\n",
       "      <td>00:47</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>91</td>\n",
       "      <td>0.228056</td>\n",
       "      <td>0.226158</td>\n",
       "      <td>0.228884</td>\n",
       "      <td>00:47</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>92</td>\n",
       "      <td>0.229393</td>\n",
       "      <td>0.259765</td>\n",
       "      <td>0.379430</td>\n",
       "      <td>00:46</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>93</td>\n",
       "      <td>0.226941</td>\n",
       "      <td>0.227165</td>\n",
       "      <td>0.230044</td>\n",
       "      <td>00:47</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>94</td>\n",
       "      <td>0.226688</td>\n",
       "      <td>0.255630</td>\n",
       "      <td>0.342563</td>\n",
       "      <td>00:46</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>95</td>\n",
       "      <td>0.226634</td>\n",
       "      <td>0.258448</td>\n",
       "      <td>0.378554</td>\n",
       "      <td>00:45</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>96</td>\n",
       "      <td>0.227179</td>\n",
       "      <td>0.242040</td>\n",
       "      <td>0.276685</td>\n",
       "      <td>00:45</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>97</td>\n",
       "      <td>0.226767</td>\n",
       "      <td>0.254543</td>\n",
       "      <td>0.355904</td>\n",
       "      <td>00:45</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>98</td>\n",
       "      <td>0.226112</td>\n",
       "      <td>0.259723</td>\n",
       "      <td>0.379426</td>\n",
       "      <td>00:45</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>99</td>\n",
       "      <td>0.226464</td>\n",
       "      <td>0.258944</td>\n",
       "      <td>0.379032</td>\n",
       "      <td>00:45</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "learn.fine_tune(100,1e-3, freeze_epochs=40)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "spread-starter",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.8.5 64-bit ('fastai': conda)",
   "language": "python",
   "name": "python38564bitfastaicondad52d12c5a30a4725bf9d3e235cf1271c"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
