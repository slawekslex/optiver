{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "0ffca2f5",
   "metadata": {},
   "outputs": [],
   "source": [
    "from fastai.vision.all import *\n",
    "from fastai.tabular.all import *\n",
    "from sklearn.model_selection import KFold, GroupKFold"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "f28e7c4d",
   "metadata": {},
   "outputs": [],
   "source": [
    "PATH = Path('../input/optiver-realized-volatility-prediction')\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "ba82696f",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_ftrs = pd.read_feather('train_24cols.feather')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "9400e05c",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_ftrs['offset']=pd.read_csv(PATH/'train_with_wap.csv').offset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "c7d72b4b",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_ftrs = train_ftrs.fillna(0)\n",
    "train_ftrs['trade_seconds'] = 'more'\n",
    "\n",
    "\n",
    "\n",
    "for val in range(3): train_ftrs.loc[train_ftrs.seconds_in_bucket_size_0_600==val, 'trade_seconds'] = val"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "d287dfea",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([9.9968e-01, 1.0003e+00, 7.6999e-04, 7.6673e-04, 9.9948e-01, 1.0005e+00,\n",
      "        9.5934e-04, 9.2822e-04]) tensor([0.0037, 0.0037, 0.0054, 0.0050, 0.0037, 0.0037, 0.0067, 0.0057])\n",
      "CPU times: user 14.8 s, sys: 14.7 s, total: 29.5 s\n",
      "Wall time: 2min 3s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "torch_data = torch.load(PATH/'torch_data.pth')\n",
    "\n",
    "\n",
    "\n",
    "means, stds = torch_data.mean(dim=0), torch_data.std(dim=0)\n",
    "print(means, stds)\n",
    "torch_data = (torch_data - means) / stds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "64748387",
   "metadata": {},
   "outputs": [],
   "source": [
    "class ReadBatch(ItemTransform):\n",
    "    def encodes(self, to):\n",
    "        book_offsets = torch.tensor(to['offset'].to_numpy()).long()\n",
    "        book_data = torch_data.view(-1,600,8)[book_offsets//600,:,:]\n",
    "        book_data = book_data.permute(0,2,1)\n",
    "        res = (tensor(to.cats).long(),tensor(to.conts).float(), book_data)        \n",
    "        res = res + (tensor(to.targ),)\n",
    "        if to.device is not None: res = to_device(res, to.device)\n",
    "        return res"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "fbb4f412",
   "metadata": {},
   "outputs": [],
   "source": [
    "trn_idx, val_idx = first(GroupKFold().split(train_ftrs, groups = train_ftrs.time_id))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "6c423e9f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Index(['log_return2_std_0_600', 'stock_id', 'row_id', 'time_id', 'target',\n",
       "       'log_return_price_std_0_600', 'order_count_sum_0_600',\n",
       "       'seconds_in_bucket_size_0_600', 'size_sum_0_600',\n",
       "       'log_return1_std_0_600_min_time', 'log_return1_std_0_600_mean_time',\n",
       "       'log_return1_std_0_600_min_stock', 'log_return1_std_0_600_mean_stock',\n",
       "       'log_return1_std_0_600', 'log_return1_std_200_600',\n",
       "       'log_return1_std_400_600', 'price_spread_mean_0_600',\n",
       "       'log_return_price_std_0_600_mean_time',\n",
       "       'log_return_price_std_200_600_mean_time',\n",
       "       'log_return_price_std_400_600_mean_time',\n",
       "       'log_return_price_std_0_600_min_time',\n",
       "       'log_return_price_std_200_600_min_time',\n",
       "       'log_return_price_std_400_600_min_time', 'total_volume_mean_0_600',\n",
       "       'offset', 'trade_seconds'],\n",
       "      dtype='object')"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_ftrs.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "ece36ea1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# cols_to_keep = ['stock_id', 'row_id', 'time_id', 'target',\n",
    "#        'log_return_price_std_0_600', 'order_count_sum_0_600',\n",
    "#        'seconds_in_bucket_size_0_600', 'size_sum_0_600',\n",
    "#        'log_return1_std_0_600_min_time', 'log_return1_std_0_600_mean_time',\n",
    "#        'log_return1_std_0_600_min_stock', 'log_return1_std_0_600_mean_stock',\n",
    "#        'log_return_price_std_0_600_mean_time',\n",
    "#        'log_return_price_std_200_600_mean_time',\n",
    "#        'log_return_price_std_400_600_mean_time',\n",
    "#        'log_return_price_std_0_600_min_time',\n",
    "#        'log_return_price_std_200_600_min_time',\n",
    "#        'log_return_price_std_400_600_min_time', 'total_volume_mean_0_600',\n",
    "#        'offset', 'trade_seconds']\n",
    "\n",
    "# train_ftrs = train_ftrs[cols_to_keep]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "a70b88e9",
   "metadata": {},
   "outputs": [],
   "source": [
    "cont_nn,cat_nn = cont_cat_split(train_ftrs, max_card=9000, dep_var='target')\n",
    "cont_nn.remove('offset')\n",
    "cat_nn=[x for x in cat_nn if not x in ['row_id', 'time_id']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "7f1a9ec1",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/slex/programy/anaconda3/envs/fastai/lib/python3.8/site-packages/torch/_tensor.py:575: UserWarning: floor_divide is deprecated, and will be removed in a future version of pytorch. It currently rounds toward 0 (like the 'trunc' function NOT 'floor'). This results in incorrect rounding for negative values.\n",
      "To keep the current behavior, use torch.div(a, b, rounding_mode='trunc'), or for actual floor division, use torch.div(a, b, rounding_mode='floor'). (Triggered internally at  /opt/conda/conda-bld/pytorch_1623448278899/work/aten/src/ATen/native/BinaryOps.cpp:467.)\n",
      "  return torch.floor_divide(self, other)\n"
     ]
    }
   ],
   "source": [
    "procs_nn = [Categorify, FillMissing, Normalize]\n",
    "to_nn = TabularPandas(train_ftrs, procs_nn, cat_nn, cont_nn,\n",
    "                      splits=[list(trn_idx), list(val_idx)], y_names='target')\n",
    "dls = to_nn.dataloaders(1024, after_batch = ReadBatch)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "30e53c8b",
   "metadata": {},
   "outputs": [],
   "source": [
    "class ResnetModel(nn.Module):\n",
    "    def __init__(self):\n",
    "        self.conv_layers = None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "54065118",
   "metadata": {},
   "outputs": [],
   "source": [
    "class ConvModel(nn.Module):\n",
    "    def __init__(self, emb_szs, n_cont, layer_sizes, conv_layers, embed_p,ps):\n",
    "        super().__init__()\n",
    "        self.embeds = nn.ModuleList([Embedding(ni, nf) for ni,nf in emb_szs])\n",
    "        self.emb_drop = nn.Dropout(embed_p)\n",
    "        self.conv_layers = conv_layers\n",
    "        self.bn_cont = nn.BatchNorm1d(n_cont)\n",
    "        n_emb = sum(e.embedding_dim for e in self.embeds)\n",
    "        sizes = [n_emb + n_cont + 20] + layer_sizes + [1]\n",
    "        actns = [nn.ReLU() for _ in range(len(sizes)-2)] + [None]\n",
    "        layers = [LinBnDrop(sizes[i], sizes[i+1], bn = (i!=len(actns)-1), p=p, act=a, lin_first=True)\n",
    "                       for i,(p,a) in enumerate(zip(ps+[0.],actns))]\n",
    "        layers.append(SigmoidRange(0, 0.1))\n",
    "        self.layers = nn.Sequential(*layers)\n",
    "    def forward(self, x_cat, x_cont, x_raw):\n",
    "        x = [e(x_cat[:,i]) for i,e in enumerate(self.embeds)]\n",
    "        x = torch.cat(x, 1)\n",
    "        x = self.emb_drop(x)\n",
    "        x_cont = self.bn_cont(x_cont)\n",
    "        x_conv = self.conv_layers(x_raw)\n",
    "        x = torch.cat([x, x_cont, x_conv], 1)\n",
    "        return self.layers(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "c6f38934",
   "metadata": {},
   "outputs": [],
   "source": [
    "class ResBlock(nn.Module):\n",
    "    def __init__(self, ch):\n",
    "        super().__init__()\n",
    "        self.layers = nn.Sequential(\n",
    "            nn.Conv1d(ch, ch, kernel_size = 5, padding = 2, padding_mode='replicate'),\n",
    "            nn.BatchNorm1d(ch),\n",
    "            nn.ReLU(),\n",
    "            nn.Conv1d(ch, ch, kernel_size = 5, padding = 2, padding_mode='replicate'),\n",
    "            nn.BatchNorm1d(ch),\n",
    "        )\n",
    "        \n",
    "    def forward(self, x):\n",
    "        res = self.layers(x) + x\n",
    "        res = F.relu(res)\n",
    "        return res"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "3d9200c9",
   "metadata": {},
   "outputs": [],
   "source": [
    "conv_layers =torch.load('models/conv_model.pth', map_location='cpu').conv_layers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "cecedda0",
   "metadata": {},
   "outputs": [],
   "source": [
    "def split_2way(model):\n",
    "    #return L(params(model.initial_conv)+params(model.conv_layers), params(model.classifier))\n",
    "    return L(params(model.conv_layers), params(model.layers)+params(model.embeds))\n",
    "def rmspe(preds, targs):\n",
    "    x = (targs-preds)/targs\n",
    "    return (x**2).mean().sqrt()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "f8d4417e",
   "metadata": {},
   "outputs": [],
   "source": [
    "emb_sizes = [(len(dls.train.classes['stock_id']), 10),\n",
    "             (len(dls.train.classes['trade_seconds']), 3)]\n",
    "n_cont = len(dls.cont_names)\n",
    "layer_sizes = [400,100,400]\n",
    "embed_p = .05\n",
    "ps = [.1,.1,0]\n",
    "model = ConvModel(emb_sizes, n_cont, layer_sizes, conv_layers, embed_p,ps)\n",
    "\n",
    "learn = Learner(dls,model, loss_func=rmspe, splitter = split_2way, metrics=AccumMetric(rmspe))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "2e8c410d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: left;\">\n",
       "      <th>epoch</th>\n",
       "      <th>train_loss</th>\n",
       "      <th>valid_loss</th>\n",
       "      <th>rmspe</th>\n",
       "      <th>time</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>0</td>\n",
       "      <td>1.238774</td>\n",
       "      <td>0.675798</td>\n",
       "      <td>0.706926</td>\n",
       "      <td>00:13</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>0.278628</td>\n",
       "      <td>0.245432</td>\n",
       "      <td>0.250347</td>\n",
       "      <td>00:07</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>0.262858</td>\n",
       "      <td>0.244574</td>\n",
       "      <td>0.247372</td>\n",
       "      <td>00:07</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: left;\">\n",
       "      <th>epoch</th>\n",
       "      <th>train_loss</th>\n",
       "      <th>valid_loss</th>\n",
       "      <th>rmspe</th>\n",
       "      <th>time</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>0</td>\n",
       "      <td>0.235095</td>\n",
       "      <td>0.230788</td>\n",
       "      <td>0.233482</td>\n",
       "      <td>00:09</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>0.232812</td>\n",
       "      <td>0.230086</td>\n",
       "      <td>0.232527</td>\n",
       "      <td>00:09</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>0.230987</td>\n",
       "      <td>0.232582</td>\n",
       "      <td>0.234973</td>\n",
       "      <td>00:09</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>0.232708</td>\n",
       "      <td>0.227846</td>\n",
       "      <td>0.230702</td>\n",
       "      <td>00:09</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4</td>\n",
       "      <td>0.236114</td>\n",
       "      <td>0.231254</td>\n",
       "      <td>0.233557</td>\n",
       "      <td>00:09</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>5</td>\n",
       "      <td>0.232495</td>\n",
       "      <td>0.227626</td>\n",
       "      <td>0.230519</td>\n",
       "      <td>00:09</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>6</td>\n",
       "      <td>0.252308</td>\n",
       "      <td>0.230342</td>\n",
       "      <td>0.233176</td>\n",
       "      <td>00:09</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>7</td>\n",
       "      <td>0.269719</td>\n",
       "      <td>0.242933</td>\n",
       "      <td>0.244985</td>\n",
       "      <td>00:09</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>8</td>\n",
       "      <td>0.235807</td>\n",
       "      <td>0.232033</td>\n",
       "      <td>0.234452</td>\n",
       "      <td>00:09</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>9</td>\n",
       "      <td>0.238349</td>\n",
       "      <td>0.226086</td>\n",
       "      <td>0.228665</td>\n",
       "      <td>00:09</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>10</td>\n",
       "      <td>0.227713</td>\n",
       "      <td>0.224446</td>\n",
       "      <td>0.227088</td>\n",
       "      <td>00:09</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>11</td>\n",
       "      <td>0.239851</td>\n",
       "      <td>0.229370</td>\n",
       "      <td>0.232138</td>\n",
       "      <td>00:09</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>12</td>\n",
       "      <td>0.233526</td>\n",
       "      <td>0.224238</td>\n",
       "      <td>0.226706</td>\n",
       "      <td>00:09</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>13</td>\n",
       "      <td>0.232982</td>\n",
       "      <td>0.243298</td>\n",
       "      <td>0.279728</td>\n",
       "      <td>00:09</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>14</td>\n",
       "      <td>0.227106</td>\n",
       "      <td>0.325245</td>\n",
       "      <td>1.004949</td>\n",
       "      <td>00:09</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>15</td>\n",
       "      <td>0.231175</td>\n",
       "      <td>0.225995</td>\n",
       "      <td>0.228579</td>\n",
       "      <td>00:09</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>16</td>\n",
       "      <td>0.224176</td>\n",
       "      <td>0.220794</td>\n",
       "      <td>0.224001</td>\n",
       "      <td>00:09</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>17</td>\n",
       "      <td>0.225778</td>\n",
       "      <td>0.220905</td>\n",
       "      <td>0.223691</td>\n",
       "      <td>00:08</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>18</td>\n",
       "      <td>0.225844</td>\n",
       "      <td>0.227581</td>\n",
       "      <td>0.229949</td>\n",
       "      <td>00:08</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>19</td>\n",
       "      <td>0.221326</td>\n",
       "      <td>0.219430</td>\n",
       "      <td>0.221765</td>\n",
       "      <td>00:08</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>20</td>\n",
       "      <td>0.221313</td>\n",
       "      <td>0.240568</td>\n",
       "      <td>0.243460</td>\n",
       "      <td>00:08</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>21</td>\n",
       "      <td>0.222255</td>\n",
       "      <td>0.219897</td>\n",
       "      <td>0.222337</td>\n",
       "      <td>00:08</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>22</td>\n",
       "      <td>0.219961</td>\n",
       "      <td>0.217675</td>\n",
       "      <td>0.219827</td>\n",
       "      <td>00:08</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>23</td>\n",
       "      <td>0.229584</td>\n",
       "      <td>0.223379</td>\n",
       "      <td>0.227489</td>\n",
       "      <td>00:08</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>24</td>\n",
       "      <td>0.223438</td>\n",
       "      <td>0.223853</td>\n",
       "      <td>0.226885</td>\n",
       "      <td>00:08</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>25</td>\n",
       "      <td>0.226793</td>\n",
       "      <td>0.258626</td>\n",
       "      <td>0.424001</td>\n",
       "      <td>00:08</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>26</td>\n",
       "      <td>0.220900</td>\n",
       "      <td>0.220613</td>\n",
       "      <td>0.223216</td>\n",
       "      <td>00:08</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>27</td>\n",
       "      <td>0.228035</td>\n",
       "      <td>0.219800</td>\n",
       "      <td>0.222731</td>\n",
       "      <td>00:08</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>28</td>\n",
       "      <td>0.231726</td>\n",
       "      <td>0.228720</td>\n",
       "      <td>0.231271</td>\n",
       "      <td>00:08</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>29</td>\n",
       "      <td>0.222238</td>\n",
       "      <td>0.222199</td>\n",
       "      <td>0.224546</td>\n",
       "      <td>00:08</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>30</td>\n",
       "      <td>0.219886</td>\n",
       "      <td>0.217942</td>\n",
       "      <td>0.220577</td>\n",
       "      <td>00:08</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>31</td>\n",
       "      <td>0.221215</td>\n",
       "      <td>0.227294</td>\n",
       "      <td>0.229383</td>\n",
       "      <td>00:08</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>32</td>\n",
       "      <td>0.220295</td>\n",
       "      <td>0.222294</td>\n",
       "      <td>0.224818</td>\n",
       "      <td>00:08</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>33</td>\n",
       "      <td>0.220953</td>\n",
       "      <td>0.220530</td>\n",
       "      <td>0.223123</td>\n",
       "      <td>00:08</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>34</td>\n",
       "      <td>0.219368</td>\n",
       "      <td>0.218572</td>\n",
       "      <td>0.221428</td>\n",
       "      <td>00:08</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>35</td>\n",
       "      <td>0.218779</td>\n",
       "      <td>0.218815</td>\n",
       "      <td>0.221216</td>\n",
       "      <td>00:08</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>36</td>\n",
       "      <td>0.218860</td>\n",
       "      <td>0.374601</td>\n",
       "      <td>1.236161</td>\n",
       "      <td>00:08</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>37</td>\n",
       "      <td>0.218875</td>\n",
       "      <td>0.230938</td>\n",
       "      <td>0.267604</td>\n",
       "      <td>00:08</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>38</td>\n",
       "      <td>0.216615</td>\n",
       "      <td>0.219696</td>\n",
       "      <td>0.221970</td>\n",
       "      <td>00:08</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>39</td>\n",
       "      <td>0.218813</td>\n",
       "      <td>0.289961</td>\n",
       "      <td>0.737687</td>\n",
       "      <td>00:08</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>40</td>\n",
       "      <td>0.218959</td>\n",
       "      <td>0.466813</td>\n",
       "      <td>2.309101</td>\n",
       "      <td>00:08</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>41</td>\n",
       "      <td>0.217538</td>\n",
       "      <td>0.217323</td>\n",
       "      <td>0.220429</td>\n",
       "      <td>00:08</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>42</td>\n",
       "      <td>0.217768</td>\n",
       "      <td>0.240186</td>\n",
       "      <td>0.328641</td>\n",
       "      <td>00:08</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>43</td>\n",
       "      <td>0.217181</td>\n",
       "      <td>0.285693</td>\n",
       "      <td>0.701850</td>\n",
       "      <td>00:08</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>44</td>\n",
       "      <td>0.216320</td>\n",
       "      <td>0.279582</td>\n",
       "      <td>0.622451</td>\n",
       "      <td>00:08</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>45</td>\n",
       "      <td>0.217513</td>\n",
       "      <td>0.343202</td>\n",
       "      <td>1.209801</td>\n",
       "      <td>00:08</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>46</td>\n",
       "      <td>0.216849</td>\n",
       "      <td>0.285321</td>\n",
       "      <td>0.700447</td>\n",
       "      <td>00:08</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>47</td>\n",
       "      <td>0.217191</td>\n",
       "      <td>0.224133</td>\n",
       "      <td>0.239006</td>\n",
       "      <td>00:08</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>48</td>\n",
       "      <td>0.216473</td>\n",
       "      <td>0.235763</td>\n",
       "      <td>0.307226</td>\n",
       "      <td>00:08</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>49</td>\n",
       "      <td>0.216704</td>\n",
       "      <td>0.255914</td>\n",
       "      <td>0.443831</td>\n",
       "      <td>00:08</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "learn.fine_tune(50,5e-3, freeze_epochs=3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b2899a47",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.8.5 64-bit ('fastai': conda)",
   "language": "python",
   "name": "python38564bitfastaicondad52d12c5a30a4725bf9d3e235cf1271c"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
