{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "6047664c",
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "c2e831c9",
   "metadata": {},
   "outputs": [],
   "source": [
    "from fastai.tabular.all import *\n",
    "from sklearn.model_selection import KFold, GroupKFold\n",
    "from optuna.integration import FastAIPruningCallback\n",
    "import optuna"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "acf495ed",
   "metadata": {},
   "outputs": [],
   "source": [
    "STOCK_COUNT = 112\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "c7307406",
   "metadata": {},
   "outputs": [],
   "source": [
    "config = {\n",
    " 'block_size': 566,\n",
    " 'bottleneck': 89,\n",
    " 'emb_p': 0.22892755376131763,\n",
    " 'emb_size': 29,\n",
    " 'jit_std': 0.044428121554388224,\n",
    " 'lr': 0.0090050544233066,\n",
    " 'mask_perc': 15,\n",
    " 'multiplier0': 0.36610514833346075,\n",
    " 'multiplier1': 0.4699461659624539,\n",
    " 'multiplier2': 0.19267216205111673,\n",
    " 'multiplier3': 0.1103899309385393,\n",
    " 'p0': 0.7779388520646903,\n",
    " 'p1': 0.4268044282083482,\n",
    " 'p2': 0.017189932979278854,\n",
    " 'p3': 0.6956976227194849,\n",
    " 'time_p0': 0.040130990403726904,\n",
    " 'time_p1': 0.2795015767348503,\n",
    " 'time_p2': 0.2534561744168111,\n",
    " 'time_p3': 0.35356055022302363,\n",
    " 'wd': 0.1755624424764211\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "d3d42267",
   "metadata": {},
   "outputs": [],
   "source": [
    "def fill_missing(train_df):\n",
    "    all_times = train_df.time_id.unique()\n",
    "    all_stocks = train_df.stock_id.unique()\n",
    "    filled_df = train_df.copy()\n",
    "    filled_df=filled_df.set_index(['time_id', 'stock_id'])\n",
    "    new_index = pd.MultiIndex.from_product([all_times, all_stocks], names = ['time_id', 'stock_id'])\n",
    "    filled_df = filled_df.reindex(new_index).reset_index()\n",
    "    filled_df = filled_df.fillna(0)\n",
    "    return filled_df\n",
    "\n",
    "\n",
    "def tauify(train_df):\n",
    "    for c in train_df.columns:\n",
    "        if 'sum' in c: train_df[c] = np.sqrt(1/(train_df[c]+1))\n",
    "    return train_df\n",
    "\n",
    "def post_process(train_df, time_windows,  do_tau):\n",
    "    train_df = fill_missing(train_df)\n",
    "    \n",
    "    if do_tau: train_df = tauify(train_df)\n",
    "    return train_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "f0f8af9b",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Jitter(ItemTransform):\n",
    "    def __init__(self, jit_std):\n",
    "            super().__init__()\n",
    "            self.split_idx = 0\n",
    "            self.jit_std = jit_std\n",
    "            \n",
    "    def encodes(self, b):\n",
    "        #print('doing jitter ', self.jit_std)\n",
    "        jitter = torch.empty_like(b[1]).normal_(0, self.jit_std)\n",
    "        b[1] += jitter\n",
    "        return b\n",
    "\n",
    "class MaskTfm(ItemTransform):\n",
    "    \n",
    "    def __init__(self, mask_perc):\n",
    "        super().__init__()\n",
    "        self.split_idx = 0\n",
    "        self.mask_perc = mask_perc\n",
    "    \n",
    "    def mask(self, x, indices):\n",
    "        x[torch.tensor(indices, device=x.device)] = 0\n",
    "        return x\n",
    "    \n",
    "    def encodes(self, x):\n",
    "        #print('doing mask', self.mask_perc)\n",
    "        n = len(x[0])\n",
    "        to_mask = (n * self.mask_perc) // 100\n",
    "        indices = np.random.choice(np.array(range(n)), to_mask, replace=False)\n",
    "        x = [self.mask(y, indices) for y in x]\n",
    "        \n",
    "        return x\n",
    "\n",
    "class MyDataLoader(TabDataLoader):\n",
    "    def __init__(self, dataset, jit_std, mask_perc, bs=16, shuffle=False, after_batch=None, num_workers=0,  **kwargs):\n",
    "        if after_batch is None: after_batch = L(TransformBlock().batch_tfms)+ReadTabBatch(dataset) + [Jitter(jit_std), MaskTfm(mask_perc)]\n",
    "        super().__init__(dataset, bs=bs, shuffle=shuffle, after_batch=after_batch, num_workers=num_workers, **kwargs)\n",
    "\n",
    "    def shuffle_fn(self, idxs):\n",
    "        idxs = np.array(idxs).reshape(-1,112)\n",
    "        np.random.shuffle(idxs)\n",
    "        return idxs.reshape(-1).tolist()\n",
    "\n",
    "def get_dls(train_df, bs, trn_idx, val_idx, jit_std=.13, mask_perc=8):\n",
    "    cont_nn,cat_nn = cont_cat_split(train_df, max_card=9000, dep_var='target')\n",
    "    cat_nn=[x for x in cat_nn if not x in ['row_id', 'time_id']]\n",
    "    \n",
    "    procs_nn = [Categorify, Normalize]\n",
    "    to_nn = TabularPandas(train_df, procs_nn, cat_nn, cont_nn, splits=[list(trn_idx), list(val_idx)], y_names='target')\n",
    "    dls = to_nn.dataloaders(bs=112*100, shuffle=True, dl_type = MyDataLoader, jit_std=jit_std, mask_perc=mask_perc)\n",
    "    dls.train_ds.split_idx=0\n",
    "    dls.valid_ds.split_idx=1\n",
    "    return dls"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "82e72fde",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "ea24c4cc",
   "metadata": {},
   "outputs": [],
   "source": [
    "#train_df = pd.read_feather('train_24cols.feather')\n",
    "# train_df = pd.read_feather('train_126ftrs.feater')\n",
    "# train_df = fill_missing(train_df)\n",
    "# train_df = append_trade_count(train_df)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "3d3331e0",
   "metadata": {},
   "outputs": [],
   "source": [
    "class TimeEncoding(nn.Module):\n",
    "    def __init__(self, inp_size, bottleneck, p):\n",
    "        super().__init__()\n",
    "        self.multiplier  = nn.Parameter(torch.tensor(.5)) \n",
    "        self.initial_layers = LinBnDrop(inp_size, bottleneck, act=nn.ReLU(True), p=p, bn=False)\n",
    "        \n",
    "        self.concat_layers = nn.Sequential(\n",
    "            nn.BatchNorm1d(bottleneck * STOCK_COUNT),\n",
    "            nn.Linear(bottleneck * STOCK_COUNT, inp_size),\n",
    "            nn.Tanh()\n",
    "        )\n",
    "        \n",
    "    def forward(self, x):\n",
    "        y = self.initial_layers(x)\n",
    "        times = y.shape[0] // STOCK_COUNT\n",
    "        y = y.view(times, -1)\n",
    "        y = self.concat_layers(y)\n",
    "   \n",
    "        y = y.view(times,1,-1).expand(times,STOCK_COUNT,-1).contiguous().view(times*STOCK_COUNT, -1)\n",
    "        \n",
    "        return x + y * self.multiplier\n",
    "\n",
    "class BN(nn.Module):\n",
    "    def __init__(self, features):\n",
    "        super().__init__()\n",
    "        self.num_features = features\n",
    "        self.bn = nn.BatchNorm1d(STOCK_COUNT * self.num_features)\n",
    "    def forward(self, x):\n",
    "        sh = x.shape\n",
    "        x = x.view(-1, STOCK_COUNT * self.num_features)\n",
    "        x = self.bn(x)\n",
    "        return x.view(*sh)\n",
    "\n",
    "class ParallelBlock(nn.Module):\n",
    "    def __init__(self, block_size, p, time_p, bottleneck, do_skip):\n",
    "        super().__init__()\n",
    "        self.do_skip = do_skip\n",
    "        self.layers = nn.Sequential(\n",
    "            nn.Linear(block_size, block_size),\n",
    "            BN(block_size ),\n",
    "            nn.Dropout(p),\n",
    "            nn.ReLU(True),\n",
    "            TimeEncoding(block_size, bottleneck, time_p)\n",
    "        )\n",
    "    def forward(self, x):\n",
    "        y = self.layers(x)\n",
    "        if self.do_skip: return (y + x) /2\n",
    "        else: return y\n",
    "    \n",
    "class ParallelModel(nn.Module):\n",
    "    def __init__(self, inp_size, emb_sz, block_size, ps, bottleneck, time_ps, embed_p, do_skip ):\n",
    "        super().__init__()\n",
    "        \n",
    "        self.embeds = nn.Parameter(torch.empty(STOCK_COUNT, emb_sz))\n",
    "        torch.nn.init.normal_(self.embeds)\n",
    "        self.embed_drop = nn.Dropout(embed_p)\n",
    "        \n",
    "        layers = [nn.Linear(inp_size+emb_sz, block_size),\n",
    "                 BN(block_size),\n",
    "                 nn.ReLU(True)]\n",
    "        for p, time_p in zip( ps, time_ps):            \n",
    "            layers.append(ParallelBlock(block_size, p, time_p, bottleneck, do_skip))\n",
    "            \n",
    "        layers.append(nn.Linear(block_size, 1))\n",
    "        layers.append(SigmoidRange(0, .1))\n",
    "        self.layers = nn.Sequential(*layers)\n",
    "    \n",
    "    \n",
    "    def forward(self, x_cat, x_cont):\n",
    "        bs = x_cont.shape[0]//STOCK_COUNT\n",
    "        emb = self.embeds.expand(bs, *self.embeds.shape)\n",
    "    \n",
    "        emb = self.embed_drop(emb)\n",
    "        x_cont = x_cont.view(bs, STOCK_COUNT, -1)\n",
    "        x = torch.cat([x_cont, emb], dim=2)\n",
    "        x = x.view(bs*STOCK_COUNT, -1)\n",
    "        return self.layers(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "df3f0521",
   "metadata": {},
   "outputs": [],
   "source": [
    "def rmspe(preds, targs):\n",
    "    mask = targs != 0\n",
    "    targs, preds = torch.masked_select(targs, mask), torch.masked_select(preds, mask)\n",
    "    x = (targs-preds)/targs\n",
    "    res= (x**2).mean().sqrt()\n",
    "    if torch.isnan(res): \n",
    "        print(targs)\n",
    "        print(preds)\n",
    "        raise Exception('fck loss is nan')\n",
    "    return res"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "f2454b2f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train( train_df, trn_idx=None, val_idx=None, save_as=None):\n",
    "    \n",
    "    jit_std=config['jit_std']\n",
    "    mask_perc=config['mask_perc']\n",
    "    \n",
    "    \n",
    "    if trn_idx is None:\n",
    "        trn_idx, val_idx = first(GroupKFold().split(train_df, groups = train_df.time_id))\n",
    "    dls = get_dls(train_df, 100, trn_idx, val_idx, jit_std=jit_std, mask_perc = mask_perc)\n",
    "    inp_size = len(dls.cont_names)\n",
    "    \n",
    "    do_skip = True\n",
    "    emb_size = config['emb_size']\n",
    "   \n",
    "    emb_p = config[f'emb_p']\n",
    "    block_size = config['block_size']\n",
    "    ps = [config[f'p{i}'] for i in range(4)]\n",
    "    bottleneck = config['bottleneck']\n",
    "    time_ps = [config[f'time_p{i}'] for i in range(4)]\n",
    "    multipliers = [config[f'multiplier{i}'] for i in range(4)]\n",
    "    \n",
    "    lr = config['lr']\n",
    "    wd = config['wd']\n",
    "    \n",
    "    \n",
    "    model = ParallelModel(inp_size, emb_size, block_size, ps, bottleneck, time_ps, emb_p, do_skip)\n",
    "    learn = Learner(dls,model = model, loss_func=rmspe, metrics=AccumMetric(rmspe), opt_func=ranger,wd=wd).to_fp16()\n",
    "    \n",
    "    learn.fit_flat_cos(50, lr)\n",
    "    if save_as:\n",
    "        learn.save(save_as)\n",
    "    return  L(learn.recorder.values).itemgot(2)[-1]\n",
    "    \n",
    "\n",
    "def train_cross_valid(trial, train_df, save_as=None):\n",
    "    res = 0\n",
    "    do_subtract = False#trial.suggest_categorical('do_subtract', [True, False])\n",
    "    do_append = False#trial.suggest_categorical('do_append', [True, False])\n",
    "    do_tau = True#trial.suggest_categorical('do_tau', [True, False])\n",
    "    train_df = post_process(train_df, time_windows, do_subtract, do_append, do_tau)\n",
    "    splits = GroupKFold().split(train_df, groups = train_df.time_id)\n",
    "    for idx, (trn_idx, val_idx) in enumerate(splits):\n",
    "        v = train(trial, train_df, trn_idx, val_idx, save_as + str(idx) if save_as else None)\n",
    "        print(f'fold {idx}: {v}')\n",
    "        res +=v;\n",
    "    return res/5"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4ff25f2b",
   "metadata": {},
   "source": [
    "## Generate data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "78ac68f1",
   "metadata": {},
   "outputs": [],
   "source": [
    "from optiver_features import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "6d422f2b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 2 µs, sys: 0 ns, total: 2 µs\n",
      "Wall time: 3.58 µs\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "# book_feature_dict = {\n",
    "#     wap1: [np.mean, np.std, 'nunique'],\n",
    "#     wap2: [np.mean, np.std],\n",
    "#     log_return1: [np.std],\n",
    "#     log_return2: [np.std],\n",
    "#     ask_spread: [np.mean, np.std],\n",
    "#     price_spread:[np.mean, np.std],\n",
    "#     total_volume:[np.mean, np.std],\n",
    "# }\n",
    "# trade_feature_dict = {\n",
    "#         log_return_price: [np.std, np.mean],\n",
    "#         'seconds_in_bucket':[np.size],\n",
    "#         'size':[np.sum],\n",
    "#         'order_count':[np.sum],\n",
    "# }\n",
    "\n",
    "time_windows = [(0,600), (0,100), (100,200), (200,300), (300,400), (400, 500), (500,600)]\n",
    "# ofg = OptiverFeatureGenerator(book_feature_dict, trade_feature_dict, time_windows)\n",
    "# train_df = ofg.generate_train_df()\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "032d9ba4",
   "metadata": {},
   "outputs": [],
   "source": [
    "#train_df.to_feather('train_141cols.feather')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "55058cb0",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_df = pd.read_feather('train_141cols.feather')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "2f286ed0",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_df = post_process(train_df, time_windows, True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0e95f67d",
   "metadata": {},
   "source": [
    "## Train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "77650b22",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: left;\">\n",
       "      <th>epoch</th>\n",
       "      <th>train_loss</th>\n",
       "      <th>valid_loss</th>\n",
       "      <th>rmspe</th>\n",
       "      <th>time</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>0</td>\n",
       "      <td>3.582103</td>\n",
       "      <td>0.809872</td>\n",
       "      <td>0.810512</td>\n",
       "      <td>00:01</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>1.718419</td>\n",
       "      <td>0.566879</td>\n",
       "      <td>0.567700</td>\n",
       "      <td>00:02</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>1.117208</td>\n",
       "      <td>0.541401</td>\n",
       "      <td>0.541874</td>\n",
       "      <td>00:02</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>0.852414</td>\n",
       "      <td>0.504048</td>\n",
       "      <td>0.504526</td>\n",
       "      <td>00:02</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4</td>\n",
       "      <td>0.714223</td>\n",
       "      <td>0.478481</td>\n",
       "      <td>0.478873</td>\n",
       "      <td>00:02</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>5</td>\n",
       "      <td>0.629962</td>\n",
       "      <td>0.409632</td>\n",
       "      <td>0.410347</td>\n",
       "      <td>00:02</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>6</td>\n",
       "      <td>0.568250</td>\n",
       "      <td>0.403058</td>\n",
       "      <td>0.403800</td>\n",
       "      <td>00:02</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>7</td>\n",
       "      <td>0.534728</td>\n",
       "      <td>0.437080</td>\n",
       "      <td>0.437478</td>\n",
       "      <td>00:02</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>8</td>\n",
       "      <td>0.530645</td>\n",
       "      <td>0.392528</td>\n",
       "      <td>0.393109</td>\n",
       "      <td>00:02</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>9</td>\n",
       "      <td>0.503139</td>\n",
       "      <td>0.407431</td>\n",
       "      <td>0.408438</td>\n",
       "      <td>00:01</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>10</td>\n",
       "      <td>0.490178</td>\n",
       "      <td>0.373910</td>\n",
       "      <td>0.374561</td>\n",
       "      <td>00:02</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>11</td>\n",
       "      <td>0.486809</td>\n",
       "      <td>0.360919</td>\n",
       "      <td>0.361567</td>\n",
       "      <td>00:02</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>12</td>\n",
       "      <td>0.461378</td>\n",
       "      <td>0.339420</td>\n",
       "      <td>0.340002</td>\n",
       "      <td>00:02</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>13</td>\n",
       "      <td>0.435569</td>\n",
       "      <td>0.343032</td>\n",
       "      <td>0.343729</td>\n",
       "      <td>00:02</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>14</td>\n",
       "      <td>0.429723</td>\n",
       "      <td>0.362263</td>\n",
       "      <td>0.362795</td>\n",
       "      <td>00:02</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>15</td>\n",
       "      <td>0.428206</td>\n",
       "      <td>0.323299</td>\n",
       "      <td>0.323737</td>\n",
       "      <td>00:01</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>16</td>\n",
       "      <td>0.413951</td>\n",
       "      <td>0.297892</td>\n",
       "      <td>0.298250</td>\n",
       "      <td>00:01</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>17</td>\n",
       "      <td>0.384022</td>\n",
       "      <td>0.294049</td>\n",
       "      <td>0.294542</td>\n",
       "      <td>00:02</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>18</td>\n",
       "      <td>0.364061</td>\n",
       "      <td>0.295187</td>\n",
       "      <td>0.295758</td>\n",
       "      <td>00:02</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>19</td>\n",
       "      <td>0.349170</td>\n",
       "      <td>0.279420</td>\n",
       "      <td>0.279747</td>\n",
       "      <td>00:02</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>20</td>\n",
       "      <td>0.334845</td>\n",
       "      <td>0.262615</td>\n",
       "      <td>0.262951</td>\n",
       "      <td>00:02</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>21</td>\n",
       "      <td>0.322068</td>\n",
       "      <td>0.260770</td>\n",
       "      <td>0.261126</td>\n",
       "      <td>00:02</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>22</td>\n",
       "      <td>0.316447</td>\n",
       "      <td>0.268428</td>\n",
       "      <td>0.268877</td>\n",
       "      <td>00:02</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>23</td>\n",
       "      <td>0.312025</td>\n",
       "      <td>0.260423</td>\n",
       "      <td>0.260777</td>\n",
       "      <td>00:02</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>24</td>\n",
       "      <td>0.307477</td>\n",
       "      <td>0.259592</td>\n",
       "      <td>0.259975</td>\n",
       "      <td>00:02</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>25</td>\n",
       "      <td>0.301421</td>\n",
       "      <td>0.279046</td>\n",
       "      <td>0.279516</td>\n",
       "      <td>00:02</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>26</td>\n",
       "      <td>0.299837</td>\n",
       "      <td>0.249112</td>\n",
       "      <td>0.249492</td>\n",
       "      <td>00:02</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>27</td>\n",
       "      <td>0.292597</td>\n",
       "      <td>0.247971</td>\n",
       "      <td>0.248460</td>\n",
       "      <td>00:02</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>28</td>\n",
       "      <td>0.290112</td>\n",
       "      <td>0.253925</td>\n",
       "      <td>0.254572</td>\n",
       "      <td>00:02</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>29</td>\n",
       "      <td>0.285096</td>\n",
       "      <td>0.249791</td>\n",
       "      <td>0.249957</td>\n",
       "      <td>00:02</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>30</td>\n",
       "      <td>0.281206</td>\n",
       "      <td>0.235555</td>\n",
       "      <td>0.235682</td>\n",
       "      <td>00:02</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>31</td>\n",
       "      <td>0.266815</td>\n",
       "      <td>0.229593</td>\n",
       "      <td>0.229678</td>\n",
       "      <td>00:02</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>32</td>\n",
       "      <td>0.265331</td>\n",
       "      <td>0.221800</td>\n",
       "      <td>0.221838</td>\n",
       "      <td>00:02</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>33</td>\n",
       "      <td>0.258397</td>\n",
       "      <td>0.217581</td>\n",
       "      <td>0.217661</td>\n",
       "      <td>00:02</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>34</td>\n",
       "      <td>0.258566</td>\n",
       "      <td>0.219557</td>\n",
       "      <td>0.219582</td>\n",
       "      <td>00:02</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>35</td>\n",
       "      <td>0.250199</td>\n",
       "      <td>0.226906</td>\n",
       "      <td>0.226973</td>\n",
       "      <td>00:02</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>36</td>\n",
       "      <td>0.242983</td>\n",
       "      <td>0.216579</td>\n",
       "      <td>0.216631</td>\n",
       "      <td>00:02</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>37</td>\n",
       "      <td>0.244573</td>\n",
       "      <td>0.234045</td>\n",
       "      <td>0.234060</td>\n",
       "      <td>00:02</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>38</td>\n",
       "      <td>0.244997</td>\n",
       "      <td>0.221303</td>\n",
       "      <td>0.221408</td>\n",
       "      <td>00:02</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>39</td>\n",
       "      <td>0.242506</td>\n",
       "      <td>0.216282</td>\n",
       "      <td>0.216320</td>\n",
       "      <td>00:02</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>40</td>\n",
       "      <td>0.236723</td>\n",
       "      <td>0.226932</td>\n",
       "      <td>0.227142</td>\n",
       "      <td>00:02</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>41</td>\n",
       "      <td>0.233972</td>\n",
       "      <td>0.215693</td>\n",
       "      <td>0.215751</td>\n",
       "      <td>00:02</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>42</td>\n",
       "      <td>0.230431</td>\n",
       "      <td>0.213817</td>\n",
       "      <td>0.213861</td>\n",
       "      <td>00:02</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>43</td>\n",
       "      <td>0.230489</td>\n",
       "      <td>0.215459</td>\n",
       "      <td>0.215515</td>\n",
       "      <td>00:02</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>44</td>\n",
       "      <td>0.226780</td>\n",
       "      <td>0.213557</td>\n",
       "      <td>0.213621</td>\n",
       "      <td>00:02</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>45</td>\n",
       "      <td>0.228242</td>\n",
       "      <td>0.215545</td>\n",
       "      <td>0.215629</td>\n",
       "      <td>00:02</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>46</td>\n",
       "      <td>0.224348</td>\n",
       "      <td>0.212957</td>\n",
       "      <td>0.213014</td>\n",
       "      <td>00:02</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>47</td>\n",
       "      <td>0.220284</td>\n",
       "      <td>0.212781</td>\n",
       "      <td>0.212851</td>\n",
       "      <td>00:02</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>48</td>\n",
       "      <td>0.217373</td>\n",
       "      <td>0.213070</td>\n",
       "      <td>0.213135</td>\n",
       "      <td>00:02</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>49</td>\n",
       "      <td>0.215242</td>\n",
       "      <td>0.213203</td>\n",
       "      <td>0.213285</td>\n",
       "      <td>00:02</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "0.21328474581241608"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train(train_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "da5d3466",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.8.5 64-bit ('fastai': conda)",
   "language": "python",
   "name": "python38564bitfastaicondad52d12c5a30a4725bf9d3e235cf1271c"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
