{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "41c424e4",
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "feb21401",
   "metadata": {},
   "outputs": [],
   "source": [
    "from fastai.tabular.all import *\n",
    "from sklearn.model_selection import KFold, GroupKFold\n",
    "from optuna.integration import FastAIPruningCallback\n",
    "import optuna"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "83296f81",
   "metadata": {},
   "outputs": [],
   "source": [
    "STOCK_COUNT = 112\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "557890d8",
   "metadata": {},
   "outputs": [],
   "source": [
    "config = {\n",
    " 'block_size': 566,\n",
    " 'bottleneck': 89,\n",
    " 'emb_p': 0.22892755376131763,\n",
    " 'emb_size': 29,\n",
    " 'jit_std': 0.044428121554388224,\n",
    " 'lr': 0.0090050544233066,\n",
    " 'mask_perc': 15,\n",
    " 'multiplier0': 0.36610514833346075,\n",
    " 'multiplier1': 0.4699461659624539,\n",
    " 'multiplier2': 0.19267216205111673,\n",
    " 'multiplier3': 0.1103899309385393,\n",
    " 'p0': 0.7779388520646903,\n",
    " 'p1': 0.4268044282083482,\n",
    " 'p2': 0.017189932979278854,\n",
    " 'p3': 0.6956976227194849,\n",
    " 'time_p0': 0.040130990403726904,\n",
    " 'time_p1': 0.2795015767348503,\n",
    " 'time_p2': 0.2534561744168111,\n",
    " 'time_p3': 0.35356055022302363,\n",
    " 'wd': 0.1755624424764211\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "f0e31647",
   "metadata": {},
   "outputs": [],
   "source": [
    "def fill_missing(train_df):\n",
    "    all_times = train_df.time_id.unique()\n",
    "    all_stocks = train_df.stock_id.unique()\n",
    "    filled_df = train_df.copy()\n",
    "    filled_df=filled_df.set_index(['time_id', 'stock_id'])\n",
    "    new_index = pd.MultiIndex.from_product([all_times, all_stocks], names = ['time_id', 'stock_id'])\n",
    "    filled_df = filled_df.reindex(new_index).reset_index()\n",
    "    filled_df = filled_df.fillna(0)\n",
    "    return filled_df\n",
    "\n",
    "\n",
    "def tauify(train_df):\n",
    "    for c in train_df.columns:\n",
    "        if 'sum' in c: train_df[c] = np.sqrt(1/(train_df[c]+1))\n",
    "    return train_df\n",
    "\n",
    "def post_process(train_df, time_windows,  do_tau):\n",
    "    train_df = fill_missing(train_df)\n",
    "    \n",
    "    if do_tau: train_df = tauify(train_df)\n",
    "    return train_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "5cf94821",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Jitter(ItemTransform):\n",
    "    def __init__(self, jit_std):\n",
    "            super().__init__()\n",
    "            self.split_idx = 0\n",
    "            self.jit_std = jit_std\n",
    "            \n",
    "    def encodes(self, b):\n",
    "        #print('doing jitter ', self.jit_std)\n",
    "        jitter = torch.empty_like(b[1]).normal_(0, self.jit_std)\n",
    "        b[1] += jitter\n",
    "        return b\n",
    "\n",
    "class MaskTfm(ItemTransform):\n",
    "    \n",
    "    def __init__(self, mask_perc):\n",
    "        super().__init__()\n",
    "        self.split_idx = 0\n",
    "        self.mask_perc = mask_perc\n",
    "    \n",
    "    def mask(self, x, indices):\n",
    "        x[torch.tensor(indices, device=x.device)] = 0\n",
    "        return x\n",
    "    \n",
    "    def encodes(self, x):\n",
    "        #print('doing mask', self.mask_perc)\n",
    "        n = len(x[0])\n",
    "        to_mask = (n * self.mask_perc) // 100\n",
    "        indices = np.random.choice(np.array(range(n)), to_mask, replace=False)\n",
    "        x = [self.mask(y, indices) for y in x]\n",
    "        \n",
    "        return x\n",
    "\n",
    "class MyDataLoader(TabDataLoader):\n",
    "    def __init__(self, dataset, jit_std, mask_perc, bs=16, shuffle=False, after_batch=None, num_workers=0,  **kwargs):\n",
    "        if after_batch is None: after_batch = L(TransformBlock().batch_tfms)+ReadTabBatch(dataset) + [Jitter(jit_std), MaskTfm(mask_perc)]\n",
    "        super().__init__(dataset, bs=bs, shuffle=shuffle, after_batch=after_batch, num_workers=num_workers, **kwargs)\n",
    "\n",
    "    def shuffle_fn(self, idxs):\n",
    "        idxs = np.array(idxs).reshape(-1,112)\n",
    "        np.random.shuffle(idxs)\n",
    "        return idxs.reshape(-1).tolist()\n",
    "\n",
    "def get_dls(train_df, bs, trn_idx, val_idx, jit_std=.13, mask_perc=8):\n",
    "    cont_nn,cat_nn = cont_cat_split(train_df, max_card=9000, dep_var='target')\n",
    "    cat_nn=[x for x in cat_nn if not x in ['row_id', 'time_id']]\n",
    "    \n",
    "    procs_nn = [Categorify, Normalize]\n",
    "    to_nn = TabularPandas(train_df, procs_nn, cat_nn, cont_nn, splits=[list(trn_idx), list(val_idx)], y_names='target')\n",
    "    dls = to_nn.dataloaders(bs=112*100, shuffle=True, dl_type = MyDataLoader, jit_std=jit_std, mask_perc=mask_perc)\n",
    "    dls.train_ds.split_idx=0\n",
    "    dls.valid_ds.split_idx=1\n",
    "    return dls"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b07969c2",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "4630d2da",
   "metadata": {},
   "outputs": [],
   "source": [
    "class TimeEncoding(nn.Module):\n",
    "    def __init__(self, inp_size, bottleneck, p):\n",
    "        super().__init__()\n",
    "        self.multiplier  = nn.Parameter(torch.tensor(.5)) \n",
    "        self.initial_layers = LinBnDrop(inp_size, bottleneck, act=nn.ReLU(True), p=p, bn=False)\n",
    "        \n",
    "        self.concat_layers = nn.Sequential(\n",
    "            nn.BatchNorm1d(bottleneck * STOCK_COUNT),\n",
    "            nn.Linear(bottleneck * STOCK_COUNT, inp_size),\n",
    "            nn.Tanh()\n",
    "        )\n",
    "        \n",
    "    def forward(self, x):\n",
    "        y = self.initial_layers(x)\n",
    "        y = y.view(x.shape[0], -1)\n",
    "        y = self.concat_layers(y)\n",
    "        y = y[:,None,:]\n",
    "        y = y.expand(*x.shape)\n",
    "        \n",
    "        res = x + y * self.multiplier\n",
    "        return res\n",
    "\n",
    "class BN(nn.Module):\n",
    "    def __init__(self, features):\n",
    "        super().__init__()\n",
    "        self.num_features = features\n",
    "        self.bn = nn.BatchNorm1d(STOCK_COUNT * self.num_features)\n",
    "    def forward(self, x):\n",
    "        sh = x.shape\n",
    "        x = x.view(-1, STOCK_COUNT * self.num_features)\n",
    "        x = self.bn(x)\n",
    "        return x.view(*sh)\n",
    "\n",
    "class ParallelBlock(nn.Module):\n",
    "    def __init__(self, block_size, p, time_p, bottleneck, do_skip):\n",
    "        super().__init__()\n",
    "        self.do_skip = do_skip\n",
    "        self.layers = nn.Sequential(\n",
    "            nn.Linear(block_size, block_size),\n",
    "            nn.BatchNorm1d(STOCK_COUNT),\n",
    "            nn.Dropout(p),\n",
    "            nn.ReLU(True),\n",
    "            TimeEncoding(block_size, bottleneck, time_p)\n",
    "        )\n",
    "    def forward(self, x):\n",
    "        y = self.layers(x)\n",
    "        if self.do_skip: return (y + x) /2\n",
    "        else: return y\n",
    "    \n",
    "class ParallelModel(nn.Module):\n",
    "    def __init__(self, inp_size, emb_sz, block_size, ps, bottleneck, time_ps, embed_p, do_skip ):\n",
    "        super().__init__()\n",
    "        \n",
    "        self.embeds = nn.Parameter(torch.empty(STOCK_COUNT, emb_sz))\n",
    "        torch.nn.init.normal_(self.embeds)\n",
    "        self.embed_drop = nn.Dropout(embed_p)\n",
    "        \n",
    "        layers = [nn.Linear(inp_size+emb_sz, block_size),\n",
    "                 nn.BatchNorm1d(STOCK_COUNT),\n",
    "                 nn.ReLU(True)]\n",
    "        for p, time_p in zip( ps, time_ps):            \n",
    "            layers.append(ParallelBlock(block_size, p, time_p, bottleneck, do_skip))\n",
    "            \n",
    "        layers.append(nn.Linear(block_size, 1))\n",
    "        layers.append(SigmoidRange(0, .1))\n",
    "        self.layers = nn.Sequential(*layers)\n",
    "    \n",
    "    \n",
    "    def forward(self, x_cat, x_cont):\n",
    "        bs = x_cont.shape[0]//STOCK_COUNT\n",
    "        emb = self.embeds.expand(bs, *self.embeds.shape)\n",
    "    \n",
    "        emb = self.embed_drop(emb)\n",
    "        x_cont = x_cont.view(bs, STOCK_COUNT, -1)\n",
    "        x = torch.cat([x_cont, emb], dim=2)\n",
    "        #x = x.view(bs*STOCK_COUNT, -1)\n",
    "        res = self.layers(x)\n",
    "        return res.view(bs * STOCK_COUNT, -1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "7aeb27d9",
   "metadata": {},
   "outputs": [],
   "source": [
    "def rmspe(preds, targs):\n",
    "    mask = targs != 0\n",
    "    targs, preds = torch.masked_select(targs, mask), torch.masked_select(preds, mask)\n",
    "    x = (targs-preds)/targs\n",
    "    res= (x**2).mean().sqrt()\n",
    "    if torch.isnan(res): \n",
    "        print(targs)\n",
    "        print(preds)\n",
    "        raise Exception('fck loss is nan')\n",
    "    return res"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "7bfa8f95",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train( train_df, trn_idx=None, val_idx=None, save_as=None):\n",
    "    \n",
    "    jit_std=config['jit_std']\n",
    "    mask_perc=config['mask_perc']\n",
    "    \n",
    "    \n",
    "    if trn_idx is None:\n",
    "        trn_idx, val_idx = first(GroupKFold().split(train_df, groups = train_df.time_id))\n",
    "    dls = get_dls(train_df, 100, trn_idx, val_idx, jit_std=jit_std, mask_perc = mask_perc)\n",
    "    inp_size = len(dls.cont_names)\n",
    "    \n",
    "    do_skip = True\n",
    "    emb_size = config['emb_size']\n",
    "   \n",
    "    emb_p = config[f'emb_p']\n",
    "    block_size = config['block_size']\n",
    "    ps = [config[f'p{i}'] for i in range(4)]\n",
    "    bottleneck = config['bottleneck']\n",
    "    time_ps = [config[f'time_p{i}'] for i in range(4)]\n",
    "    multipliers = [config[f'multiplier{i}'] for i in range(4)]\n",
    "    \n",
    "    lr = config['lr']\n",
    "    wd = config['wd']\n",
    "    \n",
    "    \n",
    "    model = ParallelModel(inp_size, emb_size, block_size, ps, bottleneck, time_ps, emb_p, do_skip)\n",
    "    learn = Learner(dls,model = model, loss_func=rmspe, metrics=AccumMetric(rmspe), opt_func=ranger,wd=wd).to_fp16()\n",
    "    \n",
    "    learn.fit_flat_cos(50, lr)\n",
    "    if save_as:\n",
    "        learn.save(save_as)\n",
    "    return  L(learn.recorder.values).itemgot(2)[-1]\n",
    "    \n",
    "\n",
    "def train_cross_valid(trial, train_df, save_as=None):\n",
    "    res = 0\n",
    "    do_subtract = False#trial.suggest_categorical('do_subtract', [True, False])\n",
    "    do_append = False#trial.suggest_categorical('do_append', [True, False])\n",
    "    do_tau = True#trial.suggest_categorical('do_tau', [True, False])\n",
    "    train_df = post_process(train_df, time_windows, do_subtract, do_append, do_tau)\n",
    "    splits = GroupKFold().split(train_df, groups = train_df.time_id)\n",
    "    for idx, (trn_idx, val_idx) in enumerate(splits):\n",
    "        v = train(trial, train_df, trn_idx, val_idx, save_as + str(idx) if save_as else None)\n",
    "        print(f'fold {idx}: {v}')\n",
    "        res +=v;\n",
    "    return res/5"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6d0e03bc",
   "metadata": {},
   "source": [
    "## Generate data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "7bdef6fb",
   "metadata": {},
   "outputs": [],
   "source": [
    "from optiver_features import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "035eb8fb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 1 µs, sys: 1 µs, total: 2 µs\n",
      "Wall time: 2.86 µs\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "# book_feature_dict = {\n",
    "#     wap1: [np.mean, np.std, 'nunique'],\n",
    "#     wap2: [np.mean, np.std],\n",
    "#     log_return1: [np.std],\n",
    "#     log_return2: [np.std],\n",
    "#     ask_spread: [np.mean, np.std],\n",
    "#     price_spread:[np.mean, np.std],\n",
    "#     total_volume:[np.mean, np.std],\n",
    "# }\n",
    "# trade_feature_dict = {\n",
    "#         log_return_price: [np.std, np.mean],\n",
    "#         'seconds_in_bucket':[np.size],\n",
    "#         'size':[np.sum],\n",
    "#         'order_count':[np.sum],\n",
    "# }\n",
    "\n",
    "time_windows = [(0,600), (0,100), (100,200), (200,300), (300,400), (400, 500), (500,600)]\n",
    "# ofg = OptiverFeatureGenerator(book_feature_dict, trade_feature_dict, time_windows)\n",
    "# train_df = ofg.generate_train_df()\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "5be638b3",
   "metadata": {},
   "outputs": [],
   "source": [
    "#train_df.to_feather('train_141cols.feather')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "d7a6d1b0",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_df = pd.read_feather('train_141cols.feather')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "132774b4",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_df = post_process(train_df, time_windows, True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a9607868",
   "metadata": {},
   "source": [
    "## Train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "f23311d1",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: left;\">\n",
       "      <th>epoch</th>\n",
       "      <th>train_loss</th>\n",
       "      <th>valid_loss</th>\n",
       "      <th>rmspe</th>\n",
       "      <th>time</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>0</td>\n",
       "      <td>3.576260</td>\n",
       "      <td>0.999999</td>\n",
       "      <td>0.999999</td>\n",
       "      <td>00:01</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>1.909047</td>\n",
       "      <td>0.999975</td>\n",
       "      <td>0.999975</td>\n",
       "      <td>00:01</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>1.415565</td>\n",
       "      <td>0.999889</td>\n",
       "      <td>0.999889</td>\n",
       "      <td>00:02</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>1.189189</td>\n",
       "      <td>0.866020</td>\n",
       "      <td>0.866241</td>\n",
       "      <td>00:02</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4</td>\n",
       "      <td>0.938609</td>\n",
       "      <td>0.738190</td>\n",
       "      <td>0.738609</td>\n",
       "      <td>00:02</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>5</td>\n",
       "      <td>0.754622</td>\n",
       "      <td>0.891389</td>\n",
       "      <td>0.891622</td>\n",
       "      <td>00:01</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>6</td>\n",
       "      <td>0.729951</td>\n",
       "      <td>0.772894</td>\n",
       "      <td>0.773611</td>\n",
       "      <td>00:01</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>7</td>\n",
       "      <td>0.710692</td>\n",
       "      <td>0.667879</td>\n",
       "      <td>0.668607</td>\n",
       "      <td>00:01</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>8</td>\n",
       "      <td>0.681824</td>\n",
       "      <td>0.610163</td>\n",
       "      <td>0.611529</td>\n",
       "      <td>00:02</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>9</td>\n",
       "      <td>0.635866</td>\n",
       "      <td>0.435701</td>\n",
       "      <td>0.436139</td>\n",
       "      <td>00:01</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>10</td>\n",
       "      <td>0.583071</td>\n",
       "      <td>0.448148</td>\n",
       "      <td>0.448407</td>\n",
       "      <td>00:01</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>11</td>\n",
       "      <td>0.530413</td>\n",
       "      <td>0.364624</td>\n",
       "      <td>0.364838</td>\n",
       "      <td>00:01</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>12</td>\n",
       "      <td>0.439957</td>\n",
       "      <td>0.283181</td>\n",
       "      <td>0.283279</td>\n",
       "      <td>00:01</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>13</td>\n",
       "      <td>0.376179</td>\n",
       "      <td>0.262965</td>\n",
       "      <td>0.263033</td>\n",
       "      <td>00:01</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>14</td>\n",
       "      <td>0.337605</td>\n",
       "      <td>0.253890</td>\n",
       "      <td>0.253911</td>\n",
       "      <td>00:01</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>15</td>\n",
       "      <td>0.314084</td>\n",
       "      <td>0.257915</td>\n",
       "      <td>0.257958</td>\n",
       "      <td>00:01</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>16</td>\n",
       "      <td>0.301151</td>\n",
       "      <td>0.277251</td>\n",
       "      <td>0.277355</td>\n",
       "      <td>00:01</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>17</td>\n",
       "      <td>0.292501</td>\n",
       "      <td>0.250768</td>\n",
       "      <td>0.250915</td>\n",
       "      <td>00:01</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>18</td>\n",
       "      <td>0.294715</td>\n",
       "      <td>0.320636</td>\n",
       "      <td>0.320774</td>\n",
       "      <td>00:01</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>19</td>\n",
       "      <td>0.294994</td>\n",
       "      <td>0.266050</td>\n",
       "      <td>0.266227</td>\n",
       "      <td>00:01</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>20</td>\n",
       "      <td>0.289171</td>\n",
       "      <td>0.252925</td>\n",
       "      <td>0.253025</td>\n",
       "      <td>00:01</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>21</td>\n",
       "      <td>0.279463</td>\n",
       "      <td>0.316922</td>\n",
       "      <td>0.317275</td>\n",
       "      <td>00:01</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>22</td>\n",
       "      <td>0.278797</td>\n",
       "      <td>0.256578</td>\n",
       "      <td>0.256599</td>\n",
       "      <td>00:01</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>23</td>\n",
       "      <td>0.271060</td>\n",
       "      <td>0.258704</td>\n",
       "      <td>0.258828</td>\n",
       "      <td>00:02</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>24</td>\n",
       "      <td>0.264548</td>\n",
       "      <td>0.250191</td>\n",
       "      <td>0.250384</td>\n",
       "      <td>00:02</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>25</td>\n",
       "      <td>0.262560</td>\n",
       "      <td>0.236946</td>\n",
       "      <td>0.236953</td>\n",
       "      <td>00:01</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>26</td>\n",
       "      <td>0.257870</td>\n",
       "      <td>0.230349</td>\n",
       "      <td>0.230420</td>\n",
       "      <td>00:02</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>27</td>\n",
       "      <td>0.251802</td>\n",
       "      <td>0.233940</td>\n",
       "      <td>0.234096</td>\n",
       "      <td>00:01</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>28</td>\n",
       "      <td>0.247573</td>\n",
       "      <td>0.228363</td>\n",
       "      <td>0.228397</td>\n",
       "      <td>00:02</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>29</td>\n",
       "      <td>0.248481</td>\n",
       "      <td>0.237584</td>\n",
       "      <td>0.237629</td>\n",
       "      <td>00:01</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>30</td>\n",
       "      <td>0.247073</td>\n",
       "      <td>0.234639</td>\n",
       "      <td>0.234680</td>\n",
       "      <td>00:01</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>31</td>\n",
       "      <td>0.244687</td>\n",
       "      <td>0.238190</td>\n",
       "      <td>0.238291</td>\n",
       "      <td>00:01</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>32</td>\n",
       "      <td>0.241069</td>\n",
       "      <td>0.232136</td>\n",
       "      <td>0.232241</td>\n",
       "      <td>00:02</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>33</td>\n",
       "      <td>0.245368</td>\n",
       "      <td>0.268201</td>\n",
       "      <td>0.268226</td>\n",
       "      <td>00:01</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>34</td>\n",
       "      <td>0.250889</td>\n",
       "      <td>0.228409</td>\n",
       "      <td>0.228439</td>\n",
       "      <td>00:01</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>35</td>\n",
       "      <td>0.246968</td>\n",
       "      <td>0.228632</td>\n",
       "      <td>0.228686</td>\n",
       "      <td>00:02</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>36</td>\n",
       "      <td>0.244195</td>\n",
       "      <td>0.260565</td>\n",
       "      <td>0.260577</td>\n",
       "      <td>00:01</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>37</td>\n",
       "      <td>0.247497</td>\n",
       "      <td>0.260541</td>\n",
       "      <td>0.260722</td>\n",
       "      <td>00:01</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>38</td>\n",
       "      <td>0.242407</td>\n",
       "      <td>0.221459</td>\n",
       "      <td>0.221491</td>\n",
       "      <td>00:02</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>39</td>\n",
       "      <td>0.236355</td>\n",
       "      <td>0.238152</td>\n",
       "      <td>0.238278</td>\n",
       "      <td>00:02</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>40</td>\n",
       "      <td>0.235323</td>\n",
       "      <td>0.224651</td>\n",
       "      <td>0.224678</td>\n",
       "      <td>00:01</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>41</td>\n",
       "      <td>0.233405</td>\n",
       "      <td>0.226537</td>\n",
       "      <td>0.226561</td>\n",
       "      <td>00:02</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>42</td>\n",
       "      <td>0.229532</td>\n",
       "      <td>0.217735</td>\n",
       "      <td>0.217770</td>\n",
       "      <td>00:01</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>43</td>\n",
       "      <td>0.227615</td>\n",
       "      <td>0.231990</td>\n",
       "      <td>0.232088</td>\n",
       "      <td>00:01</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>44</td>\n",
       "      <td>0.226968</td>\n",
       "      <td>0.215607</td>\n",
       "      <td>0.215645</td>\n",
       "      <td>00:01</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>45</td>\n",
       "      <td>0.224411</td>\n",
       "      <td>0.213658</td>\n",
       "      <td>0.213691</td>\n",
       "      <td>00:01</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>46</td>\n",
       "      <td>0.221762</td>\n",
       "      <td>0.216291</td>\n",
       "      <td>0.216361</td>\n",
       "      <td>00:02</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>47</td>\n",
       "      <td>0.220405</td>\n",
       "      <td>0.213419</td>\n",
       "      <td>0.213463</td>\n",
       "      <td>00:02</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>48</td>\n",
       "      <td>0.218733</td>\n",
       "      <td>0.213564</td>\n",
       "      <td>0.213611</td>\n",
       "      <td>00:02</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>49</td>\n",
       "      <td>0.217905</td>\n",
       "      <td>0.212956</td>\n",
       "      <td>0.213003</td>\n",
       "      <td>00:02</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "0.21300294995307922"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train(train_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a7604f23",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.8.5 64-bit ('fastai': conda)",
   "language": "python",
   "name": "python38564bitfastaicondad52d12c5a30a4725bf9d3e235cf1271c"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
